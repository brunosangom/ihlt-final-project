{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "Best Pearson Correlation on Validation Set: 0.8347182824177146\n",
      "Pearson Correlation on Testing Set: 0.674980276226439\n",
      "WARNING:tensorflow:From C:\\Users\\sanch\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('Features/train/lexicalFeatures_train.csv')\n",
    "test_df = pd.read_csv('Features/test/lexicalFeatures_test.csv')\n",
    "\n",
    "X = df.drop(columns=['gs']).values\n",
    "y = df['gs'].values\n",
    "\n",
    "X_test = test_df.drop(columns=['gs']).values\n",
    "y_test = test_df['gs'].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define model configurations\n",
    "def create_model(input_dim, config):\n",
    "    model = Sequential()\n",
    "    model.add(Input((input_dim,)))\n",
    "    \n",
    "    # Add hidden layers based on configuration\n",
    "    for units in config['hidden_layers']:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        if config['dropout'] > 0:\n",
    "            model.add(Dropout(config['dropout']))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Choose optimizer\n",
    "    optimizer = Adam(learning_rate=config['learning_rate']) if config['optimizer'] == 'adam' else \\\n",
    "                RMSprop(learning_rate=config['learning_rate'])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Model configurations to try\n",
    "configurations = [\n",
    "    {\n",
    "        'hidden_layers': [128, 64],\n",
    "        'dropout': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'optimizer': 'adam',\n",
    "        'epochs': 50\n",
    "    },\n",
    "    {\n",
    "        'hidden_layers': [256, 128, 64],\n",
    "        'dropout': 0.3,\n",
    "        'learning_rate': 0.0005,\n",
    "        'optimizer': 'rmsprop',\n",
    "        'epochs': 75\n",
    "    },\n",
    "    {\n",
    "        'hidden_layers': [64],\n",
    "        'dropout': 0,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'adam',\n",
    "        'epochs': 30\n",
    "    }\n",
    "]\n",
    "\n",
    "# n-fold cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "best_model = None\n",
    "best_pearson = -np.inf\n",
    "\n",
    "# Try different configurations\n",
    "for config in configurations:\n",
    "    config_pearson_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_scaled):\n",
    "        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Create the model for each fold\n",
    "        model = create_model(X_train.shape[1], config)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=config['epochs'], batch_size=32, verbose=0)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate the Pearson correlation\n",
    "        corr, _ = pearsonr(y_val, y_pred.flatten())\n",
    "        config_pearson_scores.append(corr)\n",
    "    \n",
    "    # Average Pearson correlation for this configuration\n",
    "    mean_corr = np.mean(config_pearson_scores)\n",
    "    \n",
    "    # Save the best model\n",
    "    if mean_corr > best_pearson:\n",
    "        best_pearson = mean_corr\n",
    "        best_model = create_model(X_scaled.shape[1], config)\n",
    "        best_model.fit(X_scaled, y, epochs=config['epochs'], batch_size=32, verbose=0)\n",
    "\n",
    "# Test the best model on the separate testing data\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "test_corr, _ = pearsonr(y_test, y_test_pred.flatten())\n",
    "\n",
    "print(f'Best Pearson Correlation on Validation Set: {best_pearson}')\n",
    "print(f'Pearson Correlation on Testing Set: {test_corr}')\n",
    "\n",
    "# Clear the session to free memory\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
