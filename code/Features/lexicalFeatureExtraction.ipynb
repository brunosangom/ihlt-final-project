{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.wsd import lesk\n",
    "from collections import Counter\n",
    "import math\n",
    "from itertools import chain\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet_ic', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the Information Content (IC) corpus\n",
    "ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...  4.00  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  3.75  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...  2.80  \n",
       "3  [The, American, Anglican, Council, ,, which, r...  3.40  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...  2.40  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = preprocessor.load_dataset('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[source, close, sale, said, vivendi, keeping, ...</td>\n",
       "      <td>[source, close, sale, said, vivendi, keeping, ...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[micron, declared, first, quarterly, profit, t...</td>\n",
       "      <td>[micron, number, also, marked, first, quarterl...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fine, part, failed, republican, effort, force...</td>\n",
       "      <td>[perry, said, back, senate, effort, including,...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[american, anglican, council, represents, epis...</td>\n",
       "      <td>[american, anglican, council, represents, epis...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, nasdaq, composite, rose, 20.96, ...</td>\n",
       "      <td>[technology-laced, nasdaq, composite, index, i...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [source, close, sale, said, vivendi, keeping, ...   \n",
       "1  [micron, declared, first, quarterly, profit, t...   \n",
       "2  [fine, part, failed, republican, effort, force...   \n",
       "3  [american, anglican, council, represents, epis...   \n",
       "4  [tech-loaded, nasdaq, composite, rose, 20.96, ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [source, close, sale, said, vivendi, keeping, ...  4.00  \n",
       "1  [micron, number, also, marked, first, quarterl...  3.75  \n",
       "2  [perry, said, back, senate, effort, including,...  2.80  \n",
       "3  [american, anglican, council, represents, epis...  3.40  \n",
       "4  [technology-laced, nasdaq, composite, index, i...  2.40  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the text\n",
    "normal_train_df = preprocessor.remove_punctuation(train_df)\n",
    "normal_train_df = preprocessor.convert_to_lowercase(normal_train_df)\n",
    "normal_train_df = preprocessor.remove_empty_strings(normal_train_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_train_df = preprocessor.remove_stopwords(normal_train_df)\n",
    "lemmas_train_df = preprocessor.lemmatize(sw_train_df)\n",
    "\n",
    "lemmas_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[reservoir, close, sales_agreement, state, viv...</td>\n",
       "      <td>[reservoir, close, sales_agreement, pronounce,...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[micrometer, stated, first_base, every_quarter...</td>\n",
       "      <td>[micrometer, phone_number, besides, set, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fine, function, fail, republican, elbow_greas...</td>\n",
       "      <td>[perry, order, backward, United_States_Senate,...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[American, Anglican, council, represent, Episc...</td>\n",
       "      <td>[American, Anglican, council, represent, Episc...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, National_Association_of_Securiti...</td>\n",
       "      <td>[technology-laced, National_Association_of_Sec...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [reservoir, close, sales_agreement, state, viv...   \n",
       "1  [micrometer, stated, first_base, every_quarter...   \n",
       "2  [fine, function, fail, republican, elbow_greas...   \n",
       "3  [American, Anglican, council, represent, Episc...   \n",
       "4  [tech-loaded, National_Association_of_Securiti...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [reservoir, close, sales_agreement, pronounce,...  4.00  \n",
       "1  [micrometer, phone_number, besides, set, first...  3.75  \n",
       "2  [perry, order, backward, United_States_Senate,...  2.80  \n",
       "3  [American, Anglican, council, represent, Episc...  3.40  \n",
       "4  [technology-laced, National_Association_of_Sec...  2.40  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Sense Disambiguation\n",
    "\n",
    "# Substitute words in a sentence based on Lesk WSD.\n",
    "def substitute_words(processed_tokens, original_tokens):\n",
    "    substituted_sentence = []\n",
    "    for word in processed_tokens:\n",
    "        # Apply Lesk algorithm using the original sentence context\n",
    "        synset = lesk(original_tokens, word)\n",
    "        if synset:\n",
    "            # Replace with the first synonym that isn't the original word\n",
    "            substitutes = [lemma.name() for lemma in synset.lemmas() if lemma.name() != word]\n",
    "            substituted_sentence.append(substitutes[0] if substitutes else word)\n",
    "        else:\n",
    "            substituted_sentence.append(word)  # No substitution if no synset found\n",
    "    return substituted_sentence\n",
    "\n",
    "# Apply Lesk WSD to the sentences in 'df', substituting words with the most probable match in the synset.\n",
    "# The original sentences are provided in 'context'.\n",
    "def lesk_wsd(df, context):\n",
    "    wsd_df = pd.DataFrame()\n",
    "\n",
    "    # Apply the substitution to each sentence\n",
    "    wsd_df['0'] = [substitute_words(df['0'][i], context['0'][i]) for i in range(len(df))]\n",
    "    wsd_df['1'] = [substitute_words(df['1'][i], context['1'][i]) for i in range(len(df))]\n",
    "    wsd_df['gs'] = df['gs']\n",
    "    \n",
    "    return wsd_df\n",
    "\n",
    "\n",
    "wsd_train_df = lesk_wsd(lemmas_train_df, normal_train_df)\n",
    "wsd_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the 3 DataFrames into a Dictionary, with their \"names\"\n",
    "train_dfs = {'normal': normal_train_df, 'sw': sw_train_df, 'lemmas': lemmas_train_df, 'wsd': wsd_train_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features DataFrame\n",
    "train_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_jaccard</th>\n",
       "      <th>sw_jaccard</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>wsd_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normal_jaccard  sw_jaccard  lemmas_jaccard  wsd_jaccard\n",
       "0        0.533333    0.473684        0.473684     0.400000\n",
       "1        0.388889    0.500000        0.500000     0.384615\n",
       "2        0.333333    0.357143        0.357143     0.266667\n",
       "3        0.607143    0.611111        0.611111     0.318182\n",
       "4        0.192308    0.150000        0.150000     0.150000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard distance\n",
    "# The first 3 features are the jaccard similarity between the sentence pairs.\n",
    "for name, df in train_dfs.items():\n",
    "    train_features_df[f'{name}_jaccard'] = [1 - jaccard_distance(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_jaccard</th>\n",
       "      <th>sw_jaccard</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>wsd_jaccard</th>\n",
       "      <th>normal_containment</th>\n",
       "      <th>sw_containment</th>\n",
       "      <th>lemmas_containment</th>\n",
       "      <th>wsd_containment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normal_jaccard  sw_jaccard  lemmas_jaccard  wsd_jaccard  \\\n",
       "0        0.533333    0.473684        0.473684     0.400000   \n",
       "1        0.388889    0.500000        0.500000     0.384615   \n",
       "2        0.333333    0.357143        0.357143     0.266667   \n",
       "3        0.607143    0.611111        0.611111     0.318182   \n",
       "4        0.192308    0.150000        0.150000     0.150000   \n",
       "\n",
       "   normal_containment  sw_containment  lemmas_containment  wsd_containment  \n",
       "0            0.761905        0.750000            0.750000         0.666667  \n",
       "1            0.700000        0.857143            0.857143         0.714286  \n",
       "2            0.500000        0.555556            0.555556         0.444444  \n",
       "3            0.944444        0.916667            0.916667         0.583333  \n",
       "4            0.357143        0.272727            0.272727         0.272727  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Containment Measure\n",
    "# Apart from the Jaccard distance, we also measure the containment measure (Broder, 1997)\n",
    "def containment_measure(set_a, set_b):\n",
    "    # Calculate the intersection of both sets\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    \n",
    "    # Return the containment measure\n",
    "    return len(intersection) / min(len(set_a), len(set_b))\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    train_features_df[f'{name}_containment'] = [containment_measure(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word n-grams\n",
    "# That was the case for individual words (1-grams), let us now also calculate the same measures for the general n-grams\n",
    "def jaccard_similarity_ngram(sentence1, sentence2, n):\n",
    "    # Generate n-grams for both sentences\n",
    "    ngrams1 = set(ngrams(sentence1, n))\n",
    "    ngrams2 = set(ngrams(sentence2, n))\n",
    "\n",
    "    # Handle the case when one or both sentences are too short to have any n-grams\n",
    "    if not ngrams1 and not ngrams2:\n",
    "        return 1  # Consider them identical if both are too short\n",
    "    elif not ngrams1 or not ngrams2:\n",
    "        return 0  # No overlap if one is too short\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    return 1 - jaccard_distance(ngrams1, ngrams2)\n",
    "\n",
    "\n",
    "def containment_measure_ngram(sentence1, sentence2, n):\n",
    "    # Generate n-grams for both sentences\n",
    "    ngrams1 = set(ngrams(sentence1, n))\n",
    "    ngrams2 = set(ngrams(sentence2, n))\n",
    "    \n",
    "    # Handle the case when one or both sentences are too short to have any n-grams\n",
    "    if not ngrams1 and not ngrams2:\n",
    "        return 1  # Consider them identical if both are too short\n",
    "    elif not ngrams1 or not ngrams2:\n",
    "        return 0  # No overlap if one is too short\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    return containment_measure(ngrams1, ngrams2)\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    for n in range(2, 5):\n",
    "        train_features_df[f'{name}_jaccard_{n}gram'] = [jaccard_similarity_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "        train_features_df[f'{name}_containment_{n}gram'] = [containment_measure_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Word Similarity\n",
    "\n",
    "# Compute IDF weights for a DataFrame.\n",
    "def compute_idf_weights(df):\n",
    "    tokens = list(chain.from_iterable(df[['0', '1']].values.flatten()))\n",
    "    token_counts = Counter(tokens)\n",
    "    total_docs = len(tokens)\n",
    "    return {word: math.log(total_docs / (count + 1)) for word, count in token_counts.items()}\n",
    "\n",
    "# Compute word similarity using WordNet.\n",
    "def word_similarity(word1, word2, similarity_measure):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0  # Return 0 if either word has no synsets\n",
    "\n",
    "    max_similarity = 0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = similarity_measure(syn1, syn2)\n",
    "                max_similarity = max(max_similarity, sim)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if max_similarity > 20:\n",
    "        return 20 # Truncate big values to avoid numerical overflow\n",
    "    return max_similarity\n",
    "\n",
    "# Calculate pairwise word similarity of a sentence with respect to other\n",
    "# We use the aggregation strategy by Mihalcea et al. (2006)\n",
    "def directional_similarity(src_tokens, target_tokens, similarity_measure, idf_weights):\n",
    "    weighted_similarities = []\n",
    "    for w in src_tokens:\n",
    "        w_similarities = [word_similarity(w, target, similarity_measure) for target in target_tokens]\n",
    "        weighted_w_similarity = max(w_similarities) * idf_weights.get(w, 0)\n",
    "        weighted_similarities.append(weighted_w_similarity)\n",
    "    numerator = sum(weighted_similarities)\n",
    "    denominator = sum(idf_weights.get(w, 0) for w in src_tokens)\n",
    "    return numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "# Compute sentence similarity of 2 sentences, averaging their directional sentence similarities\n",
    "def sentence_similarity(t1_tokens, t2_tokens, similarity_measure, idf_weights):\n",
    "    sim_t1_to_t2 = directional_similarity(t1_tokens, t2_tokens, similarity_measure, idf_weights)\n",
    "    sim_t2_to_t1 = directional_similarity(t2_tokens, t1_tokens, similarity_measure, idf_weights)\n",
    "    \n",
    "    return 0.5 * (sim_t1_to_t2 + sim_t2_to_t1)\n",
    "\n",
    "# Define a pipeline to compute Pairwise Word Similarity using parallelization\n",
    "def compute_pairwise_word_similarities(name, df, features_df):\n",
    "    idf_weights = compute_idf_weights(df)\n",
    "\n",
    "    def compute_similarity(indexed_pair, similarity_function):\n",
    "        _, sentence_pair = indexed_pair\n",
    "        return sentence_similarity(sentence_pair['0'], sentence_pair['1'], similarity_function, idf_weights)\n",
    "\n",
    "    # Resnik similarity\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        resnik_results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda indexed_pair: compute_similarity(indexed_pair, lambda syn1, syn2: syn1.res_similarity(syn2, ic)),\n",
    "                df.iterrows()\n",
    "            ),\n",
    "            total=len(df),\n",
    "            desc=f\"Computing {name} Resnik similarity\"\n",
    "        ))\n",
    "    features_df[f'{name}_resnik_similarity'] = resnik_results\n",
    "\n",
    "    # Normalize Resnik similarity\n",
    "    min_resnik_sim = min(resnik_results)\n",
    "    max_resnik_sim = max(resnik_results)\n",
    "    features_df[f'{name}_resnik_similarity'] = [(res - min_resnik_sim) / (max_resnik_sim - min_resnik_sim) for res in resnik_results]\n",
    "\n",
    "    # Lin similarity\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        lin_results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda indexed_pair: compute_similarity(indexed_pair, lambda syn1, syn2: syn1.lin_similarity(syn2, ic)),\n",
    "                df.iterrows()\n",
    "            ),\n",
    "            total=len(df),\n",
    "            desc=f\"Computing {name} Lin similarity\"\n",
    "        ))\n",
    "    features_df[f'{name}_lin_similarity'] = lin_results\n",
    "\n",
    "    # Normalize Lin similarity\n",
    "    min_lin_sim = min(lin_results)\n",
    "    max_lin_sim = max(lin_results)\n",
    "    features_df[f'{name}_lin_similarity'] = [(lin - min_lin_sim) / (max_lin_sim - min_lin_sim) for lin in lin_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lemmas Resnik similarity: 100%|██████████| 2234/2234 [01:41<00:00, 22.02it/s] \n",
      "Computing lemmas Lin similarity: 100%|██████████| 2234/2234 [01:40<00:00, 22.22it/s] \n",
      "Computing wsd Resnik similarity:   8%|▊         | 181/2234 [00:21<27:54,  1.23it/s] C:\\Users\\sanch\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1580: UserWarning: No WordNet synset found for pos=n at offset=7780173.\n",
      "  warnings.warn(f\"No WordNet synset found for pos={pos} at offset={offset}.\")\n",
      "Computing wsd Resnik similarity: 100%|██████████| 2234/2234 [01:09<00:00, 32.30it/s] \n",
      "Computing wsd Lin similarity: 100%|██████████| 2234/2234 [01:12<00:00, 30.65it/s] \n"
     ]
    }
   ],
   "source": [
    "for name, df in {'lemmas': lemmas_train_df, 'wsd': wsd_train_df}.items():\n",
    "    compute_pairwise_word_similarities(name, df, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df['gs'] = train_df['gs'] / 5.0\n",
    "\n",
    "train_features_df.to_csv('train/lexicalFeatures_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Notes\n",
    "Maybe delete before turn in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Variable  Correlation\n",
      "0             normal_jaccard     0.476515\n",
      "1                 sw_jaccard     0.600506\n",
      "2             lemmas_jaccard     0.610119\n",
      "3                wsd_jaccard     0.515652\n",
      "4         normal_containment     0.481160\n",
      "5             sw_containment     0.616511\n",
      "6         lemmas_containment     0.626920\n",
      "7            wsd_containment     0.541817\n",
      "8       normal_jaccard_2gram     0.334975\n",
      "9   normal_containment_2gram     0.354762\n",
      "10      normal_jaccard_3gram     0.279903\n",
      "11  normal_containment_3gram     0.286566\n",
      "12      normal_jaccard_4gram     0.291721\n",
      "13  normal_containment_4gram     0.311332\n",
      "14          sw_jaccard_2gram     0.421352\n",
      "15      sw_containment_2gram     0.443682\n",
      "16          sw_jaccard_3gram     0.155031\n",
      "17      sw_containment_3gram     0.219161\n",
      "18          sw_jaccard_4gram    -0.304575\n",
      "19      sw_containment_4gram    -0.253522\n",
      "20      lemmas_jaccard_2gram     0.425503\n",
      "21  lemmas_containment_2gram     0.448294\n",
      "22      lemmas_jaccard_3gram     0.158728\n",
      "23  lemmas_containment_3gram     0.223452\n",
      "24      lemmas_jaccard_4gram    -0.303487\n",
      "25  lemmas_containment_4gram    -0.251484\n",
      "26         wsd_jaccard_2gram     0.359828\n",
      "27     wsd_containment_2gram     0.384835\n",
      "28         wsd_jaccard_3gram     0.095141\n",
      "29     wsd_containment_3gram     0.156030\n",
      "30         wsd_jaccard_4gram    -0.320554\n",
      "31     wsd_containment_4gram    -0.285917\n",
      "32  lemmas_resnik_similarity     0.432133\n",
      "33     lemmas_lin_similarity     0.441466\n",
      "34     wsd_resnik_similarity     0.435929\n",
      "35        wsd_lin_similarity     0.481170\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = {}\n",
    "for column in train_features_df.columns:\n",
    "    corr, _ = pearsonr(train_features_df[column], train_df['gs'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "# Convert the dictionary to a DataFrame for tabular representation\n",
    "correlation_table = pd.DataFrame(list(correlations.items()), columns=['Variable', 'Correlation'])\n",
    "\n",
    "print(correlation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider removing 4grams, as they have negative correlation. Maybe that could be useful too, as \"negative examples\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Average Pearson Correlation: 0.7574049307336346\n",
      "[np.float64(0.7451099638606534), np.float64(0.7696998976066158)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "df = pd.read_csv('train/lexicalFeatures_train.csv')\n",
    "\n",
    "# Assuming 'df' is your DataFrame, and 'gs' is the target column\n",
    "X = df.drop(columns=['gs']).values\n",
    "y = df['gs'].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input((input_dim,)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # For regression, single output node\n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# n-fold cross-validation\n",
    "n_folds = 2\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "cv_pearson_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create the model for each fold\n",
    "    model = create_model(X_train.shape[1])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate the Pearson correlation\n",
    "    corr, _ = pearsonr(y_val, y_pred.flatten())  # Flatten to ensure 1D arrays\n",
    "    cv_pearson_scores.append(corr)\n",
    "\n",
    "# Calculate average Pearson correlation across folds\n",
    "avg_pearson = np.mean(cv_pearson_scores)\n",
    "print(f'Average Pearson Correlation: {avg_pearson}')\n",
    "print(cv_pearson_scores)\n",
    "\n",
    "# Optionally, clear the session to free memory\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
