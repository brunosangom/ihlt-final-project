{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.wsd import lesk\n",
    "from collections import Counter\n",
    "import math\n",
    "from itertools import chain, product\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet_ic', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the Information Content (IC) corpus\n",
    "ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...  4.00  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  3.75  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...  2.80  \n",
       "3  [The, American, Anglican, Council, ,, which, r...  3.40  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...  2.40  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = preprocessor.load_dataset('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[source, close, sale, said, vivendi, keeping, ...</td>\n",
       "      <td>[source, close, sale, said, vivendi, keeping, ...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[micron, declared, first, quarterly, profit, t...</td>\n",
       "      <td>[micron, number, also, marked, first, quarterl...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fine, part, failed, republican, effort, force...</td>\n",
       "      <td>[perry, said, back, senate, effort, including,...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[american, anglican, council, represents, epis...</td>\n",
       "      <td>[american, anglican, council, represents, epis...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, nasdaq, composite, rose, 20.96, ...</td>\n",
       "      <td>[technology-laced, nasdaq, composite, index, i...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [source, close, sale, said, vivendi, keeping, ...   \n",
       "1  [micron, declared, first, quarterly, profit, t...   \n",
       "2  [fine, part, failed, republican, effort, force...   \n",
       "3  [american, anglican, council, represents, epis...   \n",
       "4  [tech-loaded, nasdaq, composite, rose, 20.96, ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [source, close, sale, said, vivendi, keeping, ...  4.00  \n",
       "1  [micron, number, also, marked, first, quarterl...  3.75  \n",
       "2  [perry, said, back, senate, effort, including,...  2.80  \n",
       "3  [american, anglican, council, represents, epis...  3.40  \n",
       "4  [technology-laced, nasdaq, composite, index, i...  2.40  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the text\n",
    "normal_train_df = preprocessor.remove_punctuation(train_df)\n",
    "normal_train_df = preprocessor.convert_to_lowercase(normal_train_df)\n",
    "normal_train_df = preprocessor.remove_empty_strings(normal_train_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_train_df = preprocessor.remove_stopwords(normal_train_df)\n",
    "lemmas_train_df = preprocessor.lemmatize(sw_train_df)\n",
    "\n",
    "lemmas_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[reservoir, close, sales_agreement, state, viv...</td>\n",
       "      <td>[reservoir, close, sales_agreement, pronounce,...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[micrometer, stated, first_base, every_quarter...</td>\n",
       "      <td>[micrometer, phone_number, besides, set, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fine, function, fail, republican, elbow_greas...</td>\n",
       "      <td>[perry, order, backward, United_States_Senate,...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[American, Anglican, council, represent, Episc...</td>\n",
       "      <td>[American, Anglican, council, represent, Episc...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, National_Association_of_Securiti...</td>\n",
       "      <td>[technology-laced, National_Association_of_Sec...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [reservoir, close, sales_agreement, state, viv...   \n",
       "1  [micrometer, stated, first_base, every_quarter...   \n",
       "2  [fine, function, fail, republican, elbow_greas...   \n",
       "3  [American, Anglican, council, represent, Episc...   \n",
       "4  [tech-loaded, National_Association_of_Securiti...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [reservoir, close, sales_agreement, pronounce,...  4.00  \n",
       "1  [micrometer, phone_number, besides, set, first...  3.75  \n",
       "2  [perry, order, backward, United_States_Senate,...  2.80  \n",
       "3  [American, Anglican, council, represent, Episc...  3.40  \n",
       "4  [technology-laced, National_Association_of_Sec...  2.40  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Sense Disambiguation\n",
    "\n",
    "# Substitute words in a sentence based on Lesk WSD.\n",
    "def substitute_words(processed_tokens, original_tokens):\n",
    "    substituted_sentence = []\n",
    "    for word in processed_tokens:\n",
    "        # Apply Lesk algorithm using the original sentence context\n",
    "        synset = lesk(original_tokens, word)\n",
    "        if synset:\n",
    "            # Replace with the first synonym that isn't the original word\n",
    "            substitutes = [lemma.name() for lemma in synset.lemmas() if lemma.name() != word]\n",
    "            substituted_sentence.append(substitutes[0] if substitutes else word)\n",
    "        else:\n",
    "            substituted_sentence.append(word)  # No substitution if no synset found\n",
    "    return substituted_sentence\n",
    "\n",
    "# Apply Lesk WSD to the sentences in 'df', substituting words with the most probable match in the synset.\n",
    "# The original sentences are provided in 'context'.\n",
    "def lesk_wsd(df, context):\n",
    "    wsd_df = pd.DataFrame()\n",
    "\n",
    "    # Apply the substitution to each sentence\n",
    "    wsd_df['0'] = [substitute_words(df['0'][i], context['0'][i]) for i in range(len(df))]\n",
    "    wsd_df['1'] = [substitute_words(df['1'][i], context['1'][i]) for i in range(len(df))]\n",
    "    wsd_df['gs'] = df['gs']\n",
    "    \n",
    "    return wsd_df\n",
    "\n",
    "\n",
    "wsd_train_df = lesk_wsd(lemmas_train_df, normal_train_df)\n",
    "wsd_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the 4 DataFrames into a Dictionary, with their \"names\"\n",
    "train_dfs = {'normal': normal_train_df, 'sw': sw_train_df, 'lemmas': lemmas_train_df, 'wsd': wsd_train_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features DataFrame\n",
    "train_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_jaccard</th>\n",
       "      <th>sw_jaccard</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>wsd_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normal_jaccard  sw_jaccard  lemmas_jaccard  wsd_jaccard\n",
       "0        0.533333    0.473684        0.473684     0.400000\n",
       "1        0.388889    0.500000        0.500000     0.384615\n",
       "2        0.333333    0.357143        0.357143     0.266667\n",
       "3        0.607143    0.611111        0.611111     0.318182\n",
       "4        0.192308    0.150000        0.150000     0.150000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard similarity\n",
    "# The first 4 features are the jaccard similarity between the sentence pairs.\n",
    "for name, df in train_dfs.items():\n",
    "    train_features_df[f'{name}_jaccard'] = [1 - jaccard_distance(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_jaccard</th>\n",
       "      <th>sw_jaccard</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>wsd_jaccard</th>\n",
       "      <th>normal_containment</th>\n",
       "      <th>sw_containment</th>\n",
       "      <th>lemmas_containment</th>\n",
       "      <th>wsd_containment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normal_jaccard  sw_jaccard  lemmas_jaccard  wsd_jaccard  \\\n",
       "0        0.533333    0.473684        0.473684     0.400000   \n",
       "1        0.388889    0.500000        0.500000     0.384615   \n",
       "2        0.333333    0.357143        0.357143     0.266667   \n",
       "3        0.607143    0.611111        0.611111     0.318182   \n",
       "4        0.192308    0.150000        0.150000     0.150000   \n",
       "\n",
       "   normal_containment  sw_containment  lemmas_containment  wsd_containment  \n",
       "0            0.761905        0.750000            0.750000         0.666667  \n",
       "1            0.700000        0.857143            0.857143         0.714286  \n",
       "2            0.500000        0.555556            0.555556         0.444444  \n",
       "3            0.944444        0.916667            0.916667         0.583333  \n",
       "4            0.357143        0.272727            0.272727         0.272727  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Containment Measure\n",
    "# Apart from the Jaccard distance, we also measure the containment measure (Broder, 1997)\n",
    "def containment_measure(set_a, set_b):\n",
    "    # Calculate the intersection of both sets\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    \n",
    "    # Return the containment measure\n",
    "    return len(intersection) / min(len(set_a), len(set_b))\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    train_features_df[f'{name}_containment'] = [containment_measure(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word n-grams\n",
    "# That was the case for individual words (1-grams), let us now also calculate the same measures for the general n-grams\n",
    "def jaccard_similarity_ngram(sentence1, sentence2, n):\n",
    "    # Generate n-grams for both sentences\n",
    "    ngrams1 = set(ngrams(sentence1, n))\n",
    "    ngrams2 = set(ngrams(sentence2, n))\n",
    "\n",
    "    # Handle the case when one or both sentences are too short to have any n-grams\n",
    "    if not ngrams1 and not ngrams2:\n",
    "        return 1  # Consider them identical if both are too short\n",
    "    elif not ngrams1 or not ngrams2:\n",
    "        return 0  # No overlap if one is too short\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    return 1 - jaccard_distance(ngrams1, ngrams2)\n",
    "\n",
    "\n",
    "def containment_measure_ngram(sentence1, sentence2, n):\n",
    "    # Generate n-grams for both sentences\n",
    "    ngrams1 = set(ngrams(sentence1, n))\n",
    "    ngrams2 = set(ngrams(sentence2, n))\n",
    "    \n",
    "    # Handle the case when one or both sentences are too short to have any n-grams\n",
    "    if not ngrams1 and not ngrams2:\n",
    "        return 1  # Consider them identical if both are too short\n",
    "    elif not ngrams1 or not ngrams2:\n",
    "        return 0  # No overlap if one is too short\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    return containment_measure(ngrams1, ngrams2)\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    for n in range(2, 4):\n",
    "        train_features_df[f'{name}_jaccard_{n}gram'] = [jaccard_similarity_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "        train_features_df[f'{name}_containment_{n}gram'] = [containment_measure_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Word Similarity\n",
    "\n",
    "# Compute IDF weights for a DataFrame.\n",
    "def compute_idf_weights(df):\n",
    "    tokens = list(chain.from_iterable(df[['0', '1']].values.flatten()))\n",
    "    token_counts = Counter(tokens)\n",
    "    total_docs = len(tokens)\n",
    "    return {word: math.log(total_docs / (count + 1)) for word, count in token_counts.items()}\n",
    "\n",
    "\n",
    "idf_weights = compute_idf_weights(normal_train_df)\n",
    "\n",
    "# Compute word similarity using WordNet.\n",
    "def word_similarity(word1, word2, similarity_measure):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0  # Return 0 if either word has no synsets\n",
    "\n",
    "    max_similarity = 0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = similarity_measure(syn1, syn2)\n",
    "                max_similarity = max(max_similarity, sim)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if max_similarity > 20:\n",
    "        return 20 # Truncate big values to avoid numerical overflow\n",
    "    return max_similarity\n",
    "\n",
    "# Calculate pairwise word similarity of a sentence with respect to other\n",
    "# We use the aggregation strategy by Mihalcea et al. (2006)\n",
    "def directional_similarity(src_tokens, target_tokens, similarity_measure, idf_weights):\n",
    "    weighted_similarities = []\n",
    "    for w in src_tokens:\n",
    "        w_similarities = [word_similarity(w, target, similarity_measure) for target in target_tokens]\n",
    "        weighted_w_similarity = max(w_similarities) * idf_weights.get(w, 0)\n",
    "        weighted_similarities.append(weighted_w_similarity)\n",
    "    numerator = sum(weighted_similarities)\n",
    "    denominator = sum(idf_weights.get(w, 0) for w in src_tokens)\n",
    "    return numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "# Compute sentence similarity of 2 sentences, averaging their directional sentence similarities\n",
    "def sentence_similarity(t1_tokens, t2_tokens, similarity_measure, idf_weights):\n",
    "    sim_t1_to_t2 = directional_similarity(t1_tokens, t2_tokens, similarity_measure, idf_weights)\n",
    "    sim_t2_to_t1 = directional_similarity(t2_tokens, t1_tokens, similarity_measure, idf_weights)\n",
    "    \n",
    "    return 0.5 * (sim_t1_to_t2 + sim_t2_to_t1)\n",
    "\n",
    "# Define a pipeline to compute Pairwise Word Similarity using parallelization\n",
    "def compute_pairwise_word_similarities(name, df, features_df):\n",
    "    # Resnik similarity\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        resnik_results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda sentence_pair: sentence_similarity(sentence_pair['0'], sentence_pair['1'], lambda syn1, syn2: syn1.res_similarity(syn2, ic), idf_weights),\n",
    "                (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "            ),\n",
    "            total=len(df),\n",
    "            desc=f\"Computing {name} Resnik similarity\"\n",
    "        ))\n",
    "    features_df[f'{name}_resnik_similarity'] = resnik_results\n",
    "\n",
    "    # Normalize Resnik similarity\n",
    "    min_resnik_sim = min(resnik_results)\n",
    "    max_resnik_sim = max(resnik_results)\n",
    "    features_df[f'{name}_resnik_similarity'] = [(res - min_resnik_sim) / (max_resnik_sim - min_resnik_sim) for res in resnik_results]\n",
    "\n",
    "    # Lin similarity\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        lin_results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda sentence_pair: sentence_similarity(sentence_pair['0'], sentence_pair['1'], lambda syn1, syn2: syn1.lin_similarity(syn2, ic), idf_weights),\n",
    "                (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "            ),\n",
    "            total=len(df),\n",
    "            desc=f\"Computing {name} Lin similarity\"\n",
    "        ))\n",
    "    features_df[f'{name}_lin_similarity'] = lin_results\n",
    "\n",
    "    # Normalize Lin similarity\n",
    "    min_lin_sim = min(lin_results)\n",
    "    max_lin_sim = max(lin_results)\n",
    "    features_df[f'{name}_lin_similarity'] = [(lin - min_lin_sim) / (max_lin_sim - min_lin_sim) for lin in lin_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, df in {'lemmas': lemmas_train_df, 'wsd': wsd_train_df}.items():\n",
    "#     compute_pairwise_word_similarities(name, df, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet Augmented Word Overlap\n",
    "\n",
    "# Compute the maximum path similarity between two words using WordNet.\n",
    "def path_similarity(w1, w2):\n",
    "    synsets1 = wn.synsets(w1)\n",
    "    synsets2 = wn.synsets(w2)\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0.0\n",
    "    \n",
    "    max_sim = 0.0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = syn1.path_similarity(syn2, simulate_root = False)\n",
    "                if sim and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "            except:\n",
    "                continue\n",
    "    return max_sim\n",
    "\n",
    "# Compute the score of a word against a sentence.\n",
    "def score(w, S):\n",
    "    if w in S:\n",
    "        return 1\n",
    "    return max(path_similarity(w, w_prime) for w_prime in S)\n",
    "\n",
    "# Compute the WordNet-augmented coverage for two sentences S1 and S2.\n",
    "def P_WN(S1, S2):\n",
    "    S2_set = set(S2)\n",
    "    return sum(score(w, S2_set) for w in S1) / len(S2)\n",
    "\n",
    "# Compute the harmonic mean of P_WN(S1, S2) and P_WN(S2, S1).\n",
    "def wordnet_augmented_word_overlap(S1, S2):\n",
    "    P1 = P_WN(S1, S2)\n",
    "    P2 = P_WN(S2, S1)\n",
    "    \n",
    "    if P1 + P2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    return 2 * P1 * P2 / (P1 + P2)\n",
    "\n",
    "def compute_wordnet_augmented_word_overlap(dfs, features_df):\n",
    "    for name, df in dfs.items():\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(\n",
    "                    lambda sentence_pair: wordnet_augmented_word_overlap(sentence_pair['0'], sentence_pair['1']),\n",
    "                    (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                ),\n",
    "                total=len(df),\n",
    "                desc=f\"Computing WordNet-Augmented Word Overlap {name}\"\n",
    "            ))\n",
    "        features_df[f'{name}_wn_aug_overlap'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_wordnet_augmented_word_overlap({'lemmas': lemmas_train_df, 'wsd': wsd_train_df}, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Word Overlap\n",
    "\n",
    "# Extract Information Content of a synset from WordNet IC\n",
    "def information_content(synset):\n",
    "    pos = synset._pos\n",
    "    if pos != 'n' and pos != 'v':\n",
    "            return 0\n",
    "\n",
    "    icpos = ic[pos]\n",
    "\n",
    "    counts = icpos[synset._offset]\n",
    "    if counts == 0:\n",
    "        return 1e7\n",
    "    else:\n",
    "        return -math.log(counts / icpos[0])\n",
    "\n",
    "# Calculate the IC of a word through its most likely synset.\n",
    "# Returns 0 if the word is not found in WordNet.\n",
    "def calculate_ic(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return 0  # Word not in WordNet\n",
    "    synset = synsets[0]  # Use the first synset for simplicity\n",
    "    return information_content(synset)\n",
    "\n",
    "# Compute the weighted word overlap between two sentences.\n",
    "def weighted_word_overlap(s1, s2):\n",
    "    # Compute IC for words in both sentences\n",
    "    ic_words1 = {word: calculate_ic(word) for word in s1}\n",
    "    ic_words2 = {word: calculate_ic(word) for word in s2}\n",
    "    \n",
    "    # Compute WWC(S1, S2)\n",
    "    common_words = set(s1).intersection(s2)\n",
    "    numerator1 = sum(ic_words1[word] for word in common_words)\n",
    "    denominator1 = sum(ic_words2[word] for word in s2)\n",
    "    wwc_s1_s2 = numerator1 / denominator1 if denominator1 > 0 else 0\n",
    "    \n",
    "    # Compute WWC(S2, S1)\n",
    "    numerator2 = sum(ic_words2[word] for word in common_words)\n",
    "    denominator2 = sum(ic_words1[word] for word in s1)\n",
    "    wwc_s2_s1 = numerator2 / denominator2 if denominator2 > 0 else 0\n",
    "    \n",
    "    # Harmonic mean of WWC(S1, S2) and WWC(S2, S1)\n",
    "    if wwc_s1_s2 + wwc_s2_s1 == 0:\n",
    "        return 0\n",
    "    return (2 * wwc_s1_s2 * wwc_s2_s1) / (wwc_s1_s2 + wwc_s2_s1)\n",
    "\n",
    "for name, df in {'lemmas': lemmas_train_df, 'wsd': wsd_train_df}.items():\n",
    "    train_features_df[f'{name}_weighted_overlap'] = [weighted_word_overlap(sentence_pair['0'], sentence_pair['1']) for _, sentence_pair in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Lin similarity between two lemmas using WordNet.\n",
    "# Returns the maximum similarity among all possible synset pairs.\n",
    "def lin_similarity(lemma1, lemma2):\n",
    "    synsets1 = wn.synsets(lemma1)\n",
    "    synsets2 = wn.synsets(lemma2)\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0.0\n",
    "    \n",
    "    max_sim = 0.0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = syn1.lin_similarity(syn2, ic)\n",
    "                if sim and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "            except:\n",
    "                continue\n",
    "    return max_sim\n",
    "\n",
    "# Perform greedy alignment of lemmas between two sentences.\n",
    "# Returns set of aligned lemma pairs.\n",
    "def greedy_lemma_alignment(sent1, sent2):\n",
    "    # Create similarity matrix\n",
    "    similarities = np.zeros((len(sent1), len(sent2)))\n",
    "    for i, lemma1 in enumerate(sent1):\n",
    "        for j, lemma2 in enumerate(sent2):\n",
    "            similarities[i, j] = lin_similarity(lemma1, lemma2)\n",
    "    \n",
    "    # Greedily align lemmas\n",
    "    aligned_pairs = set()\n",
    "    used_indices1 = set()\n",
    "    used_indices2 = set()\n",
    "    \n",
    "    while len(used_indices1) < len(sent1) and len(used_indices2) < len(sent2):\n",
    "        # Find highest remaining similarity\n",
    "        max_sim = -1\n",
    "        max_i = -1\n",
    "        max_j = -1\n",
    "        \n",
    "        for i in range(len(sent1)):\n",
    "            if i in used_indices1:\n",
    "                continue\n",
    "            for j in range(len(sent2)):\n",
    "                if j in used_indices2:\n",
    "                    continue\n",
    "                if similarities[i, j] > max_sim:\n",
    "                    max_sim = similarities[i, j]\n",
    "                    max_i = i\n",
    "                    max_j = j\n",
    "        \n",
    "        if max_sim <= 0:\n",
    "            break\n",
    "            \n",
    "        aligned_pairs.add((sent1[max_i], sent2[max_j]))\n",
    "        used_indices1.add(max_i)\n",
    "        used_indices2.add(max_j)\n",
    "    \n",
    "    return aligned_pairs\n",
    "\n",
    "# Compute Greedy Lemma Aligning Overlap score between two sentences.\n",
    "def greedy_lemma_aligning_overlap(sent1, sent2):\n",
    "    if not sent1 or not sent2:\n",
    "        return 0.0\n",
    "        \n",
    "    # Get aligned pairs\n",
    "    aligned_pairs = greedy_lemma_alignment(sent1, sent2)\n",
    "    \n",
    "    # Compute similarity score for each pair\n",
    "    total_sim = 0.0\n",
    "    for lemma1, lemma2 in aligned_pairs:\n",
    "        # Get information content for each lemma\n",
    "        ic1 = max(information_content(syn) for syn in wn.synsets(lemma1)) if wn.synsets(lemma1) else 0\n",
    "        ic2 = max(information_content(syn) for syn in wn.synsets(lemma2)) if wn.synsets(lemma2) else 0\n",
    "        \n",
    "        # Compute semantic similarity\n",
    "        ssim = lin_similarity(lemma1, lemma2)\n",
    "        \n",
    "        # Weigh the similarity by the max IC\n",
    "        pair_sim = max(ic1, ic2) * ssim\n",
    "        total_sim += pair_sim\n",
    "    \n",
    "    # Normalize by length of longer sentence\n",
    "    normalization = max(len(sent1), len(sent2))\n",
    "    if normalization == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return total_sim / normalization\n",
    "\n",
    "def compute_greedy_lemma_aligning_overlap(dfs, features_df):\n",
    "    for name, df in dfs.items():\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(\n",
    "                    lambda sentence_pair: greedy_lemma_aligning_overlap(sentence_pair['0'], sentence_pair['1']),\n",
    "                    (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                ),\n",
    "                total=len(df),\n",
    "                desc=f\"Computing Greedy Lemma Aligning Overlap {name}\"\n",
    "            ))\n",
    "        features_df[f'{name}_glao'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Greedy Lemma Aligning Overlap lemmas: 100%|██████████| 2234/2234 [00:48<00:00, 45.94it/s] \n",
      "Computing Greedy Lemma Aligning Overlap wsd: 100%|██████████| 2234/2234 [00:37<00:00, 59.38it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_greedy_lemma_aligning_overlap({'lemmas': lemmas_train_df, 'wsd': wsd_train_df}, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df['gs'] = train_df['gs'] / 5.0\n",
    "\n",
    "train_features_df.to_csv('train/lexicalFeatures_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Pairwise Word Similarity\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m: lemmas_test_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwsd\u001b[39m\u001b[38;5;124m'\u001b[39m: wsd_test_df}\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mcompute_pairwise_word_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# WordNet Augmented Word Overlap\u001b[39;00m\n\u001b[0;32m     40\u001b[0m compute_wordnet_augmented_word_overlap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmas\u001b[39m\u001b[38;5;124m'\u001b[39m: lemmas_test_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwsd\u001b[39m\u001b[38;5;124m'\u001b[39m: wsd_test_df}, test_features_df)\n",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m, in \u001b[0;36mcompute_pairwise_word_similarities\u001b[1;34m(name, df, features_df)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_pairwise_word_similarities\u001b[39m(name, df, features_df):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Resnik similarity\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     56\u001b[0m         resnik_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tqdm(\n\u001b[1;32m---> 57\u001b[0m             \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentence_pair\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msyn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn2\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msyn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msyn2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midf_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_pair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_pair\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     61\u001b[0m             total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df),\n\u001b[0;32m     62\u001b[0m             desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Resnik similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m         ))\n\u001b[0;32m     64\u001b[0m     features_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_resnik_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m resnik_results\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Normalize Resnik similarity\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map\u001b[1;34m(self, fn, timeout, chunksize, *iterables)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    606\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;241m+\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 608\u001b[0m fs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39miterables)]\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Yield must be hidden in closure so that the futures are submitted\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;66;03m# before the first iterator value is required.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult_iterator\u001b[39m():\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py:179\u001b[0m, in \u001b[0;36mThreadPoolExecutor.submit\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m w \u001b[38;5;241m=\u001b[39m _WorkItem(f, fn, args, kwargs)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue\u001b[38;5;241m.\u001b[39mput(w)\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adjust_thread_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\concurrent\\futures\\thread.py:202\u001b[0m, in \u001b[0;36mThreadPoolExecutor._adjust_thread_count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    195\u001b[0m thread_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread_name_prefix \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    196\u001b[0m                          num_threads)\n\u001b[0;32m    197\u001b[0m t \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(name\u001b[38;5;241m=\u001b[39mthread_name, target\u001b[38;5;241m=\u001b[39m_worker,\n\u001b[0;32m    198\u001b[0m                      args\u001b[38;5;241m=\u001b[39m(weakref\u001b[38;5;241m.\u001b[39mref(\u001b[38;5;28mself\u001b[39m, weakref_cb),\n\u001b[0;32m    199\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue,\n\u001b[0;32m    200\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initializer,\n\u001b[0;32m    201\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initargs))\n\u001b[1;32m--> 202\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads\u001b[38;5;241m.\u001b[39madd(t)\n\u001b[0;32m    204\u001b[0m _threads_queues[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\threading.py:997\u001b[0m, in \u001b[0;36mThread.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m _limbo[\u001b[38;5;28mself\u001b[39m]\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the same pipeline to the testing Dataset\n",
    "\n",
    "# Load the testing dataset\n",
    "test_df = preprocessor.load_dataset('../Preprocessing/STS_test.csv')\n",
    "\n",
    "# Normalize the text\n",
    "normal_test_df = preprocessor.remove_punctuation(test_df)\n",
    "normal_test_df = preprocessor.convert_to_lowercase(normal_test_df)\n",
    "normal_test_df = preprocessor.remove_empty_strings(normal_test_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_test_df = preprocessor.remove_stopwords(normal_test_df)\n",
    "lemmas_test_df = preprocessor.lemmatize(sw_test_df)\n",
    "\n",
    "# Word Sense Disambiguation\n",
    "wsd_test_df = lesk_wsd(lemmas_test_df, normal_test_df)\n",
    "\n",
    "# Group the 4 DataFrames into a Dictionary, with their \"names\"\n",
    "test_dfs = {'normal': normal_test_df, 'sw': sw_test_df, 'lemmas': lemmas_test_df, 'wsd': wsd_test_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "test_features_df = pd.DataFrame()\n",
    "\n",
    "# Jaccard similarity and containment measure\n",
    "for name, df in test_dfs.items():\n",
    "    test_features_df[f'{name}_jaccard'] = [1 - jaccard_distance(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "    test_features_df[f'{name}_containment'] = [containment_measure(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "# Word n-grams\n",
    "for name, df in test_dfs.items():\n",
    "    for n in range(2, 4):\n",
    "        test_features_df[f'{name}_jaccard_{n}gram'] = [jaccard_similarity_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "        test_features_df[f'{name}_containment_{n}gram'] = [containment_measure_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "# Pairwise Word Similarity\n",
    "for name, df in {'lemmas': lemmas_test_df, 'wsd': wsd_test_df}.items():\n",
    "    compute_pairwise_word_similarities(name, df, test_features_df)\n",
    "\n",
    "# WordNet Augmented Word Overlap\n",
    "compute_wordnet_augmented_word_overlap({'lemmas': lemmas_test_df, 'wsd': wsd_test_df}, test_features_df)\n",
    "\n",
    "# Weighted Word Overlap\n",
    "for name, df in {'lemmas': lemmas_test_df, 'wsd': wsd_test_df}.items():\n",
    "    test_features_df[f'{name}_weighted_overlap'] = [weighted_word_overlap(sentence_pair['0'], sentence_pair['1']) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "# Greedy Lemma Aligning Overlap\n",
    "compute_greedy_lemma_aligning_overlap({'lemmas': lemmas_train_df, 'wsd': wsd_train_df}, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df['gs'] = test_df['gs'] / 5.0\n",
    "\n",
    "test_features_df.to_csv('test/lexicalFeatures_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Notes\n",
    "Maybe delete before turn in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Variable  Correlation\n",
      "0             normal_jaccard     0.476515\n",
      "1                 sw_jaccard     0.600506\n",
      "2             lemmas_jaccard     0.610119\n",
      "3                wsd_jaccard     0.515652\n",
      "4         normal_containment     0.481160\n",
      "5             sw_containment     0.616511\n",
      "6         lemmas_containment     0.626920\n",
      "7            wsd_containment     0.541817\n",
      "8       normal_jaccard_2gram     0.334975\n",
      "9   normal_containment_2gram     0.354762\n",
      "10      normal_jaccard_3gram     0.279903\n",
      "11  normal_containment_3gram     0.286566\n",
      "12          sw_jaccard_2gram     0.421352\n",
      "13      sw_containment_2gram     0.443682\n",
      "14          sw_jaccard_3gram     0.155031\n",
      "15      sw_containment_3gram     0.219161\n",
      "16      lemmas_jaccard_2gram     0.425503\n",
      "17  lemmas_containment_2gram     0.448294\n",
      "18      lemmas_jaccard_3gram     0.158728\n",
      "19  lemmas_containment_3gram     0.223452\n",
      "20         wsd_jaccard_2gram     0.359828\n",
      "21     wsd_containment_2gram     0.384835\n",
      "22         wsd_jaccard_3gram     0.095141\n",
      "23     wsd_containment_3gram     0.156030\n",
      "24  lemmas_resnik_similarity     0.420664\n",
      "25     lemmas_lin_similarity     0.424808\n",
      "26     wsd_resnik_similarity     0.338684\n",
      "27        wsd_lin_similarity     0.362654\n",
      "28     lemmas_wn_aug_overlap     0.723318\n",
      "29        wsd_wn_aug_overlap     0.658619\n",
      "30   lemmas_weighted_overlap          NaN\n",
      "31      wsd_weighted_overlap     0.406496\n",
      "32                        gs     1.000000\n",
      "33               lemmas_glao          NaN\n",
      "34                  wsd_glao          NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanch\\AppData\\Local\\Temp\\ipykernel_13208\\488710691.py:5: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(train_features_df[column], train_df['gs'])\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = {}\n",
    "for column in train_features_df.columns:\n",
    "    corr, _ = pearsonr(train_features_df[column], train_df['gs'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "# Convert the dictionary to a DataFrame for tabular representation\n",
    "correlation_table = pd.DataFrame(list(correlations.items()), columns=['Variable', 'Correlation'])\n",
    "\n",
    "print(correlation_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider removing 4grams, as they have negative correlation. Maybe that could be useful too, as \"negative examples\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step\n",
      "Best Pearson Correlation on Validation Set: 0.821299965620997\n",
      "Pearson Correlation on Testing Set: 0.5821285400465993\n",
      "WARNING:tensorflow:From C:\\Users\\sanch\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "df = pd.read_csv('train/lexicalFeatures_train.csv')\n",
    "test_df = pd.read_csv('test/lexicalFeatures_test.csv')\n",
    "\n",
    "# Assuming 'df' is the training DataFrame, 'gs' is the target column,\n",
    "# and 'test_df' is the separate testing DataFrame\n",
    "X = df.drop(columns=['gs']).values\n",
    "y = df['gs'].values\n",
    "\n",
    "X_test = test_df.drop(columns=['gs']).values\n",
    "y_test = test_df['gs'].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Input((input_dim,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))  # For regression, single output node\n",
    "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# n-fold cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "best_model = None\n",
    "best_pearson = -np.inf  # Initialize with a very low value\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_scaled):\n",
    "    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # Create the model for each fold\n",
    "    model = create_model(X_train.shape[1])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate the Pearson correlation\n",
    "    corr, _ = pearsonr(y_val, y_pred.flatten())\n",
    "    \n",
    "    # Save the model if it achieves the best Pearson correlation\n",
    "    if corr > best_pearson:\n",
    "        best_pearson = corr\n",
    "        best_model = model\n",
    "\n",
    "# Test the best model on the separate testing data\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "test_corr, _ = pearsonr(y_test, y_test_pred.flatten())\n",
    "print(f'Best Pearson Correlation on Validation Set: {best_pearson}')\n",
    "print(f'Pearson Correlation on Testing Set: {test_corr}')\n",
    "\n",
    "# Optionally, clear the session to free memory\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
