{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.wsd import lesk\n",
    "from collections import Counter\n",
    "import math\n",
    "from itertools import chain\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet_ic', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the Information Content (IC) corpus\n",
    "ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...  4.00  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  3.75  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...  2.80  \n",
       "3  [The, American, Anglican, Council, ,, which, r...  3.40  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...  2.40  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = preprocessor.load_dataset('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[source, close, sale, said, vivendi, keeping, ...</td>\n",
       "      <td>[source, close, sale, said, vivendi, keeping, ...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[micron, declared, first, quarterly, profit, t...</td>\n",
       "      <td>[micron, number, also, marked, first, quarterl...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fine, part, failed, republican, effort, force...</td>\n",
       "      <td>[perry, said, back, senate, effort, including,...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[american, anglican, council, represents, epis...</td>\n",
       "      <td>[american, anglican, council, represents, epis...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, nasdaq, composite, rose, 20.96, ...</td>\n",
       "      <td>[technology-laced, nasdaq, composite, index, i...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [source, close, sale, said, vivendi, keeping, ...   \n",
       "1  [micron, declared, first, quarterly, profit, t...   \n",
       "2  [fine, part, failed, republican, effort, force...   \n",
       "3  [american, anglican, council, represents, epis...   \n",
       "4  [tech-loaded, nasdaq, composite, rose, 20.96, ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [source, close, sale, said, vivendi, keeping, ...  4.00  \n",
       "1  [micron, number, also, marked, first, quarterl...  3.75  \n",
       "2  [perry, said, back, senate, effort, including,...  2.80  \n",
       "3  [american, anglican, council, represents, epis...  3.40  \n",
       "4  [technology-laced, nasdaq, composite, index, i...  2.40  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the text\n",
    "normal_train_df = preprocessor.remove_punctuation(train_df)\n",
    "normal_train_df = preprocessor.convert_to_lowercase(normal_train_df)\n",
    "normal_train_df = preprocessor.remove_empty_strings(normal_train_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_train_df = preprocessor.remove_stopwords(normal_train_df)\n",
    "lemmas_train_df = preprocessor.lemmatize(sw_train_df)\n",
    "\n",
    "lemmas_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[reservoir, close, sales_agreement, state, viv...</td>\n",
       "      <td>[reservoir, close, sales_agreement, pronounce,...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[micrometer, stated, first_base, every_quarter...</td>\n",
       "      <td>[micrometer, phone_number, besides, set, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fine, function, fail, republican, elbow_greas...</td>\n",
       "      <td>[perry, order, backward, United_States_Senate,...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[American, Anglican, council, represent, Episc...</td>\n",
       "      <td>[American, Anglican, council, represent, Episc...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, National_Association_of_Securiti...</td>\n",
       "      <td>[technology-laced, National_Association_of_Sec...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [reservoir, close, sales_agreement, state, viv...   \n",
       "1  [micrometer, stated, first_base, every_quarter...   \n",
       "2  [fine, function, fail, republican, elbow_greas...   \n",
       "3  [American, Anglican, council, represent, Episc...   \n",
       "4  [tech-loaded, National_Association_of_Securiti...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [reservoir, close, sales_agreement, pronounce,...  4.00  \n",
       "1  [micrometer, phone_number, besides, set, first...  3.75  \n",
       "2  [perry, order, backward, United_States_Senate,...  2.80  \n",
       "3  [American, Anglican, council, represent, Episc...  3.40  \n",
       "4  [technology-laced, National_Association_of_Sec...  2.40  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Sense Disambiguation\n",
    "\n",
    "# Substitute words in a sentence based on Lesk WSD.\n",
    "def substitute_words(processed_tokens, original_tokens):\n",
    "    substituted_sentence = []\n",
    "    for word in processed_tokens:\n",
    "        # Apply Lesk algorithm using the original sentence context\n",
    "        synset = lesk(original_tokens, word)\n",
    "        if synset:\n",
    "            # Replace with the first synonym that isn't the original word\n",
    "            substitutes = [lemma.name() for lemma in synset.lemmas() if lemma.name() != word]\n",
    "            substituted_sentence.append(substitutes[0] if substitutes else word)\n",
    "        else:\n",
    "            substituted_sentence.append(word)  # No substitution if no synset found\n",
    "    return substituted_sentence\n",
    "\n",
    "# Apply Lesk WSD to the sentences in 'df', substituting words with the most probable match in the synset.\n",
    "# The original sentences are provided in 'context'.\n",
    "def lesk_wsd(df, context):\n",
    "    wsd_df = pd.DataFrame()\n",
    "\n",
    "    # Apply the substitution to each sentence\n",
    "    wsd_df['0'] = [substitute_words(df['0'][i], context['0'][i]) for i in range(len(df))]\n",
    "    wsd_df['1'] = [substitute_words(df['1'][i], context['1'][i]) for i in range(len(df))]\n",
    "    wsd_df['gs'] = df['gs']\n",
    "    \n",
    "    return wsd_df\n",
    "\n",
    "\n",
    "wsd_train_df = lesk_wsd(lemmas_train_df, normal_train_df)\n",
    "wsd_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the 4 DataFrames into a Dictionary, with their \"names\"\n",
    "train_dfs = {'normal': normal_train_df, 'sw': sw_train_df, 'lemmas': lemmas_train_df, 'wsd': wsd_train_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "train_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_jaccard</th>\n",
       "      <th>sw_jaccard</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>wsd_jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normal_jaccard  sw_jaccard  lemmas_jaccard  wsd_jaccard\n",
       "0        0.533333    0.473684        0.473684     0.400000\n",
       "1        0.388889    0.500000        0.500000     0.384615\n",
       "2        0.333333    0.357143        0.357143     0.266667\n",
       "3        0.607143    0.611111        0.611111     0.318182\n",
       "4        0.192308    0.150000        0.150000     0.150000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jaccard similarity\n",
    "\n",
    "# The first 4 features are the jaccard similarity between the sentence pairs.\n",
    "for name, df in train_dfs.items():\n",
    "    train_features_df[f'{name}_jaccard'] = [1 - jaccard_distance(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_jaccard</th>\n",
       "      <th>sw_jaccard</th>\n",
       "      <th>lemmas_jaccard</th>\n",
       "      <th>wsd_jaccard</th>\n",
       "      <th>normal_containment</th>\n",
       "      <th>sw_containment</th>\n",
       "      <th>lemmas_containment</th>\n",
       "      <th>wsd_containment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   normal_jaccard  sw_jaccard  lemmas_jaccard  wsd_jaccard  \\\n",
       "0        0.533333    0.473684        0.473684     0.400000   \n",
       "1        0.388889    0.500000        0.500000     0.384615   \n",
       "2        0.333333    0.357143        0.357143     0.266667   \n",
       "3        0.607143    0.611111        0.611111     0.318182   \n",
       "4        0.192308    0.150000        0.150000     0.150000   \n",
       "\n",
       "   normal_containment  sw_containment  lemmas_containment  wsd_containment  \n",
       "0            0.761905        0.750000            0.750000         0.666667  \n",
       "1            0.700000        0.857143            0.857143         0.714286  \n",
       "2            0.500000        0.555556            0.555556         0.444444  \n",
       "3            0.944444        0.916667            0.916667         0.583333  \n",
       "4            0.357143        0.272727            0.272727         0.272727  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Containment Measure\n",
    "\n",
    "# Apart from the Jaccard distance, we also measure the containment measure (Broder, 1997)\n",
    "def containment_measure(set_a, set_b):\n",
    "    # Calculate the intersection of both sets\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    \n",
    "    # Return the containment measure\n",
    "    return len(intersection) / min(len(set_a), len(set_b))\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    train_features_df[f'{name}_containment'] = [containment_measure(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word n-grams\n",
    "\n",
    "# That was the case for individual words (1-grams), let us now also calculate the same measures for the general n-grams\n",
    "def jaccard_similarity_ngram(sentence1, sentence2, n):\n",
    "    # Generate n-grams for both sentences\n",
    "    ngrams1 = set(ngrams(sentence1, n))\n",
    "    ngrams2 = set(ngrams(sentence2, n))\n",
    "\n",
    "    # Handle the case when one or both sentences are too short to have any n-grams\n",
    "    if not ngrams1 and not ngrams2:\n",
    "        return 1  # Consider them identical if both are too short\n",
    "    elif not ngrams1 or not ngrams2:\n",
    "        return 0  # No overlap if one is too short\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    return 1 - jaccard_distance(ngrams1, ngrams2)\n",
    "\n",
    "\n",
    "def containment_measure_ngram(sentence1, sentence2, n):\n",
    "    # Generate n-grams for both sentences\n",
    "    ngrams1 = set(ngrams(sentence1, n))\n",
    "    ngrams2 = set(ngrams(sentence2, n))\n",
    "    \n",
    "    # Handle the case when one or both sentences are too short to have any n-grams\n",
    "    if not ngrams1 and not ngrams2:\n",
    "        return 1  # Consider them identical if both are too short\n",
    "    elif not ngrams1 or not ngrams2:\n",
    "        return 0  # No overlap if one is too short\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    return containment_measure(ngrams1, ngrams2)\n",
    "\n",
    "for name, df in train_dfs.items():\n",
    "    for n in range(2, 4):\n",
    "        train_features_df[f'{name}_jaccard_{n}gram'] = [jaccard_similarity_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "        train_features_df[f'{name}_containment_{n}gram'] = [containment_measure_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Word Similarity\n",
    "\n",
    "# Compute IDF weights for a DataFrame.\n",
    "def compute_idf_weights(df):\n",
    "    tokens = list(chain.from_iterable(df[['0', '1']].values.flatten()))\n",
    "    token_counts = Counter(tokens)\n",
    "    total_docs = len(tokens)\n",
    "    return {word: math.log(total_docs / (count + 1)) for word, count in token_counts.items()}\n",
    "\n",
    "\n",
    "idf_weights = compute_idf_weights(normal_train_df)\n",
    "\n",
    "# Compute word similarity using WordNet.\n",
    "def word_similarity(word1, word2, similarity_measure):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0  # Return 0 if either word has no synsets\n",
    "\n",
    "    max_similarity = 0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = similarity_measure(syn1, syn2)\n",
    "                max_similarity = max(max_similarity, sim)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    if max_similarity > 20:\n",
    "        return 20 # Truncate big values to avoid numerical overflow\n",
    "    return max_similarity\n",
    "\n",
    "# Calculate pairwise word similarity of a sentence with respect to other\n",
    "# We use the aggregation strategy by Mihalcea et al. (2006)\n",
    "def directional_similarity(src_tokens, target_tokens, similarity_measure, idf_weights):\n",
    "    weighted_similarities = []\n",
    "    for w in src_tokens:\n",
    "        w_similarities = [word_similarity(w, target, similarity_measure) for target in target_tokens]\n",
    "        weighted_w_similarity = max(w_similarities) * idf_weights.get(w, 0)\n",
    "        weighted_similarities.append(weighted_w_similarity)\n",
    "    numerator = sum(weighted_similarities)\n",
    "    denominator = sum(idf_weights.get(w, 0) for w in src_tokens)\n",
    "    return numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "# Compute sentence similarity of 2 sentences, averaging their directional sentence similarities\n",
    "def sentence_similarity(t1_tokens, t2_tokens, similarity_measure, idf_weights):\n",
    "    sim_t1_to_t2 = directional_similarity(t1_tokens, t2_tokens, similarity_measure, idf_weights)\n",
    "    sim_t2_to_t1 = directional_similarity(t2_tokens, t1_tokens, similarity_measure, idf_weights)\n",
    "    \n",
    "    return 0.5 * (sim_t1_to_t2 + sim_t2_to_t1)\n",
    "\n",
    "# Define a pipeline to compute Pairwise Word Similarity using parallelization\n",
    "def compute_pairwise_word_similarities(name, df, features_df):\n",
    "    # Resnik similarity\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        resnik_results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda sentence_pair: sentence_similarity(sentence_pair['0'], sentence_pair['1'], lambda syn1, syn2: syn1.res_similarity(syn2, ic), idf_weights),\n",
    "                (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "            ),\n",
    "            total=len(df),\n",
    "            desc=f\"Computing {name} Resnik similarity\"\n",
    "        ))\n",
    "    features_df[f'{name}_resnik_similarity'] = resnik_results\n",
    "\n",
    "    # Normalize Resnik similarity\n",
    "    min_resnik_sim = min(resnik_results)\n",
    "    max_resnik_sim = max(resnik_results)\n",
    "    features_df[f'{name}_resnik_similarity'] = [(res - min_resnik_sim) / (max_resnik_sim - min_resnik_sim) for res in resnik_results]\n",
    "\n",
    "    # Lin similarity\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        lin_results = list(tqdm(\n",
    "            executor.map(\n",
    "                lambda sentence_pair: sentence_similarity(sentence_pair['0'], sentence_pair['1'], lambda syn1, syn2: syn1.lin_similarity(syn2, ic), idf_weights),\n",
    "                (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "            ),\n",
    "            total=len(df),\n",
    "            desc=f\"Computing {name} Lin similarity\"\n",
    "        ))\n",
    "    features_df[f'{name}_lin_similarity'] = lin_results\n",
    "\n",
    "    # Normalize Lin similarity\n",
    "    min_lin_sim = min(lin_results)\n",
    "    max_lin_sim = max(lin_results)\n",
    "    features_df[f'{name}_lin_similarity'] = [(lin - min_lin_sim) / (max_lin_sim - min_lin_sim) for lin in lin_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lemmas Resnik similarity: 100%|██████████| 2234/2234 [03:47<00:00,  9.81it/s]\n",
      "Computing lemmas Lin similarity: 100%|██████████| 2234/2234 [01:35<00:00, 23.49it/s] \n",
      "Computing wsd Resnik similarity: 100%|██████████| 2234/2234 [00:38<00:00, 58.78it/s] \n",
      "Computing wsd Lin similarity: 100%|██████████| 2234/2234 [00:35<00:00, 62.99it/s] \n"
     ]
    }
   ],
   "source": [
    "for name, df in {'lemmas': lemmas_train_df, 'wsd': wsd_train_df}.items():\n",
    "    compute_pairwise_word_similarities(name, df, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet Augmented Word Overlap\n",
    "\n",
    "# Compute the maximum path similarity between two words using WordNet.\n",
    "def path_similarity(w1, w2):\n",
    "    synsets1 = wn.synsets(w1)\n",
    "    synsets2 = wn.synsets(w2)\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0.0\n",
    "    \n",
    "    max_sim = 0.0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = syn1.path_similarity(syn2, simulate_root = False)\n",
    "                if sim and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "            except:\n",
    "                continue\n",
    "    return max_sim\n",
    "\n",
    "# Compute the score of a word against a sentence.\n",
    "def score(w, S):\n",
    "    if w in S:\n",
    "        return 1\n",
    "    return max(path_similarity(w, w_prime) for w_prime in S)\n",
    "\n",
    "# Compute the WordNet-augmented coverage for two sentences S1 and S2.\n",
    "def P_WN(S1, S2):\n",
    "    S2_set = set(S2)\n",
    "    return sum(score(w, S2_set) for w in S1) / len(S2)\n",
    "\n",
    "# Compute the harmonic mean of P_WN(S1, S2) and P_WN(S2, S1).\n",
    "def wordnet_augmented_word_overlap(S1, S2):\n",
    "    P1 = P_WN(S1, S2)\n",
    "    P2 = P_WN(S2, S1)\n",
    "    \n",
    "    if P1 + P2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    return 2 * P1 * P2 / (P1 + P2)\n",
    "\n",
    "def compute_wordnet_augmented_word_overlap(dfs, features_df):\n",
    "    for name, df in dfs.items():\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(\n",
    "                    lambda sentence_pair: wordnet_augmented_word_overlap(sentence_pair['0'], sentence_pair['1']),\n",
    "                    (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                ),\n",
    "                total=len(df),\n",
    "                desc=f\"Computing WordNet-Augmented Word Overlap {name}\"\n",
    "            ))\n",
    "        features_df[f'{name}_wn_aug_overlap'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing WordNet-Augmented Word Overlap lemmas: 100%|██████████| 2234/2234 [03:51<00:00,  9.65it/s] \n",
      "Computing WordNet-Augmented Word Overlap wsd: 100%|██████████| 2234/2234 [02:59<00:00, 12.46it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_wordnet_augmented_word_overlap({'lemmas': lemmas_train_df, 'wsd': wsd_train_df}, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Word Overlap\n",
    "\n",
    "# Extract Information Content of a synset from WordNet IC\n",
    "def information_content(synset):\n",
    "    pos = synset._pos\n",
    "    if pos != 'n' and pos != 'v':\n",
    "            return 0\n",
    "\n",
    "    icpos = ic[pos]\n",
    "\n",
    "    counts = icpos[synset._offset]\n",
    "    if counts == 0:\n",
    "        return 1e7\n",
    "    else:\n",
    "        return -math.log(counts / icpos[0])\n",
    "\n",
    "# Calculate the IC of a word through its most likely synset.\n",
    "# Returns 0 if the word is not found in WordNet.\n",
    "def calculate_ic(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        return 0  # Word not in WordNet\n",
    "    synset = synsets[0]  # Use the first synset for simplicity\n",
    "    return information_content(synset)\n",
    "\n",
    "# Compute the weighted word overlap between two sentences.\n",
    "def weighted_word_overlap(s1, s2):\n",
    "    # Compute IC for words in both sentences\n",
    "    ic_words1 = {word: calculate_ic(word) for word in s1}\n",
    "    ic_words2 = {word: calculate_ic(word) for word in s2}\n",
    "    \n",
    "    # Compute WWC(S1, S2)\n",
    "    common_words = set(s1).intersection(s2)\n",
    "    numerator1 = sum(ic_words1[word] for word in common_words)\n",
    "    denominator1 = sum(ic_words2[word] for word in s2)\n",
    "    wwc_s1_s2 = numerator1 / denominator1 if denominator1 > 0 else 0\n",
    "    \n",
    "    # Compute WWC(S2, S1)\n",
    "    numerator2 = sum(ic_words2[word] for word in common_words)\n",
    "    denominator2 = sum(ic_words1[word] for word in s1)\n",
    "    wwc_s2_s1 = numerator2 / denominator2 if denominator2 > 0 else 0\n",
    "    \n",
    "    # Harmonic mean of WWC(S1, S2) and WWC(S2, S1)\n",
    "    if wwc_s1_s2 + wwc_s2_s1 == 0:\n",
    "        return 0\n",
    "    return (2 * wwc_s1_s2 * wwc_s2_s1) / (wwc_s1_s2 + wwc_s2_s1)\n",
    "\n",
    "for name, df in {'lemmas': lemmas_train_df, 'wsd': wsd_train_df}.items():\n",
    "    train_features_df[f'{name}_weighted_overlap'] = [weighted_word_overlap(sentence_pair['0'], sentence_pair['1']) for _, sentence_pair in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Lemma Alignment Overlap (GLAO)\n",
    "\n",
    "# Calculate Lin similarity between two lemmas using WordNet.\n",
    "# Returns the maximum similarity among all possible synset pairs.\n",
    "def lin_similarity(lemma1, lemma2):\n",
    "    synsets1 = wn.synsets(lemma1)\n",
    "    synsets2 = wn.synsets(lemma2)\n",
    "    \n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0.0\n",
    "    \n",
    "    max_sim = 0.0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            try:\n",
    "                sim = syn1.lin_similarity(syn2, ic)\n",
    "                if sim and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "            except:\n",
    "                continue\n",
    "    return max_sim\n",
    "\n",
    "# Perform greedy alignment of lemmas between two sentences.\n",
    "# Returns set of aligned lemma pairs.\n",
    "def greedy_lemma_alignment(sent1, sent2):\n",
    "    # Create similarity matrix\n",
    "    similarities = np.zeros((len(sent1), len(sent2)))\n",
    "    for i, lemma1 in enumerate(sent1):\n",
    "        for j, lemma2 in enumerate(sent2):\n",
    "            similarities[i, j] = lin_similarity(lemma1, lemma2)\n",
    "    \n",
    "    # Greedily align lemmas\n",
    "    aligned_pairs = set()\n",
    "    used_indices1 = set()\n",
    "    used_indices2 = set()\n",
    "    \n",
    "    while len(used_indices1) < len(sent1) and len(used_indices2) < len(sent2):\n",
    "        # Find highest remaining similarity\n",
    "        max_sim = -1\n",
    "        max_i = -1\n",
    "        max_j = -1\n",
    "        \n",
    "        for i in range(len(sent1)):\n",
    "            if i in used_indices1:\n",
    "                continue\n",
    "            for j in range(len(sent2)):\n",
    "                if j in used_indices2:\n",
    "                    continue\n",
    "                if similarities[i, j] > max_sim:\n",
    "                    max_sim = similarities[i, j]\n",
    "                    max_i = i\n",
    "                    max_j = j\n",
    "        \n",
    "        if max_sim <= 0:\n",
    "            break\n",
    "            \n",
    "        aligned_pairs.add((sent1[max_i], sent2[max_j]))\n",
    "        used_indices1.add(max_i)\n",
    "        used_indices2.add(max_j)\n",
    "    \n",
    "    return aligned_pairs\n",
    "\n",
    "# Compute Greedy Lemma Aligning Overlap score between two sentences.\n",
    "def greedy_lemma_aligning_overlap(sent1, sent2):\n",
    "    if not sent1 or not sent2:\n",
    "        return 0.0\n",
    "        \n",
    "    # Get aligned pairs\n",
    "    aligned_pairs = greedy_lemma_alignment(sent1, sent2)\n",
    "    \n",
    "    # Compute similarity score for each pair\n",
    "    total_sim = 0.0\n",
    "    for lemma1, lemma2 in aligned_pairs:\n",
    "        # Get information content for each lemma\n",
    "        ic1 = max(information_content(syn) for syn in wn.synsets(lemma1)) if wn.synsets(lemma1) else 0\n",
    "        ic2 = max(information_content(syn) for syn in wn.synsets(lemma2)) if wn.synsets(lemma2) else 0\n",
    "        \n",
    "        # Compute semantic similarity\n",
    "        ssim = lin_similarity(lemma1, lemma2)\n",
    "        \n",
    "        # Weigh the similarity by the max IC\n",
    "        pair_sim = max(ic1, ic2) * ssim\n",
    "        total_sim += pair_sim\n",
    "    \n",
    "    # Normalize by length of longer sentence\n",
    "    normalization = max(len(sent1), len(sent2))\n",
    "    if normalization == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    return total_sim / normalization\n",
    "\n",
    "def compute_greedy_lemma_aligning_overlap(dfs, features_df):\n",
    "    for name, df in dfs.items():\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(\n",
    "                    lambda sentence_pair: greedy_lemma_aligning_overlap(sentence_pair['0'], sentence_pair['1']),\n",
    "                    (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                ),\n",
    "                total=len(df),\n",
    "                desc=f\"Computing Greedy Lemma Aligning Overlap {name}\"\n",
    "            ))\n",
    "        features_df[f'{name}_glao'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Greedy Lemma Aligning Overlap lemmas: 100%|██████████| 2234/2234 [00:24<00:00, 90.69it/s] \n",
      "Computing Greedy Lemma Aligning Overlap wsd: 100%|██████████| 2234/2234 [00:16<00:00, 132.86it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_greedy_lemma_aligning_overlap({'lemmas': lemmas_train_df, 'wsd': wsd_train_df}, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a CSV with the Normalized Gold Standard\n",
    "train_features_df['gs'] = train_df['gs'] / 5.0\n",
    "\n",
    "train_features_df.to_csv('train/lexicalFeatures_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing lemmas Resnik similarity: 100%|██████████| 3108/3108 [00:26<00:00, 117.91it/s]\n",
      "Computing lemmas Lin similarity: 100%|██████████| 3108/3108 [01:07<00:00, 46.27it/s] \n",
      "Computing wsd Resnik similarity: 100%|██████████| 3108/3108 [00:17<00:00, 175.72it/s]\n",
      "Computing wsd Lin similarity: 100%|██████████| 3108/3108 [00:20<00:00, 154.00it/s]\n",
      "Computing WordNet-Augmented Word Overlap lemmas: 100%|██████████| 3108/3108 [02:10<00:00, 23.83it/s] \n",
      "Computing WordNet-Augmented Word Overlap wsd: 100%|██████████| 3108/3108 [01:35<00:00, 32.44it/s] \n",
      "Computing Greedy Lemma Aligning Overlap lemmas: 100%|██████████| 2234/2234 [00:26<00:00, 85.83it/s] \n",
      "Computing Greedy Lemma Aligning Overlap wsd: 100%|██████████| 2234/2234 [00:16<00:00, 133.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the same pipeline to the testing Dataset\n",
    "\n",
    "# Load the testing dataset\n",
    "test_df = preprocessor.load_dataset('../Preprocessing/STS_test.csv')\n",
    "\n",
    "# Normalize the text\n",
    "normal_test_df = preprocessor.remove_punctuation(test_df)\n",
    "normal_test_df = preprocessor.convert_to_lowercase(normal_test_df)\n",
    "normal_test_df = preprocessor.remove_empty_strings(normal_test_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_test_df = preprocessor.remove_stopwords(normal_test_df)\n",
    "lemmas_test_df = preprocessor.lemmatize(sw_test_df)\n",
    "\n",
    "# Word Sense Disambiguation\n",
    "wsd_test_df = lesk_wsd(lemmas_test_df, normal_test_df)\n",
    "\n",
    "# Group the 4 DataFrames into a Dictionary, with their \"names\"\n",
    "test_dfs = {'normal': normal_test_df, 'sw': sw_test_df, 'lemmas': lemmas_test_df, 'wsd': wsd_test_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "test_features_df = pd.DataFrame()\n",
    "\n",
    "# Jaccard similarity and containment measure\n",
    "for name, df in test_dfs.items():\n",
    "    test_features_df[f'{name}_jaccard'] = [1 - jaccard_distance(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "    test_features_df[f'{name}_containment'] = [containment_measure(set(sentence_pair['0']), set(sentence_pair['1'])) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "# Word n-grams\n",
    "for name, df in test_dfs.items():\n",
    "    for n in range(2, 4):\n",
    "        test_features_df[f'{name}_jaccard_{n}gram'] = [jaccard_similarity_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "        test_features_df[f'{name}_containment_{n}gram'] = [containment_measure_ngram(sentence_pair['0'], sentence_pair['1'], n) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "# Pairwise Word Similarity\n",
    "for name, df in {'lemmas': lemmas_test_df, 'wsd': wsd_test_df}.items():\n",
    "    compute_pairwise_word_similarities(name, df, test_features_df)\n",
    "\n",
    "# WordNet Augmented Word Overlap\n",
    "compute_wordnet_augmented_word_overlap({'lemmas': lemmas_test_df, 'wsd': wsd_test_df}, test_features_df)\n",
    "\n",
    "# Weighted Word Overlap\n",
    "for name, df in {'lemmas': lemmas_test_df, 'wsd': wsd_test_df}.items():\n",
    "    test_features_df[f'{name}_weighted_overlap'] = [weighted_word_overlap(sentence_pair['0'], sentence_pair['1']) for _, sentence_pair in df.iterrows()]\n",
    "\n",
    "# Greedy Lemma Aligning Overlap\n",
    "compute_greedy_lemma_aligning_overlap({'lemmas': lemmas_test_df, 'wsd': wsd_test_df}, test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a CSV with the Normalized Gold Standard\n",
    "test_features_df['gs'] = test_df['gs'] / 5.0\n",
    "\n",
    "test_features_df.to_csv('test/lexicalFeatures_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Variable  Correlation\n",
      "0             normal_jaccard     0.476515\n",
      "1                 sw_jaccard     0.600506\n",
      "2             lemmas_jaccard     0.610119\n",
      "3                wsd_jaccard     0.515652\n",
      "4         normal_containment     0.481160\n",
      "5             sw_containment     0.616511\n",
      "6         lemmas_containment     0.626920\n",
      "7            wsd_containment     0.541817\n",
      "8       normal_jaccard_2gram     0.334975\n",
      "9   normal_containment_2gram     0.354762\n",
      "10      normal_jaccard_3gram     0.279903\n",
      "11  normal_containment_3gram     0.286566\n",
      "12          sw_jaccard_2gram     0.421352\n",
      "13      sw_containment_2gram     0.443682\n",
      "14          sw_jaccard_3gram     0.155031\n",
      "15      sw_containment_3gram     0.219161\n",
      "16      lemmas_jaccard_2gram     0.425503\n",
      "17  lemmas_containment_2gram     0.448294\n",
      "18      lemmas_jaccard_3gram     0.158728\n",
      "19  lemmas_containment_3gram     0.223452\n",
      "20         wsd_jaccard_2gram     0.359828\n",
      "21     wsd_containment_2gram     0.384835\n",
      "22         wsd_jaccard_3gram     0.095141\n",
      "23     wsd_containment_3gram     0.156030\n",
      "24  lemmas_resnik_similarity     0.420664\n",
      "25     lemmas_lin_similarity     0.424808\n",
      "26     wsd_resnik_similarity     0.338684\n",
      "27        wsd_lin_similarity     0.362654\n",
      "28     lemmas_wn_aug_overlap     0.723318\n",
      "29        wsd_wn_aug_overlap     0.658619\n",
      "30   lemmas_weighted_overlap     0.542443\n",
      "31      wsd_weighted_overlap     0.406498\n",
      "32               lemmas_glao    -0.000939\n",
      "33                  wsd_glao     0.098976\n",
      "34                        gs     1.000000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = {}\n",
    "for column in train_features_df.columns:\n",
    "    corr, _ = pearsonr(train_features_df[column], train_df['gs'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "# Convert the dictionary to a DataFrame for tabular representation\n",
    "correlation_table = pd.DataFrame(list(correlations.items()), columns=['Variable', 'Correlation'])\n",
    "\n",
    "print(correlation_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
