{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 32.4MB/s]                    \n",
      "2024-12-09 19:05:38 INFO: Downloaded file to C:\\Users\\maric\\stanza_resources\\resources.json\n",
      "2024-12-09 19:05:38 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-12-09 19:05:39 INFO: File exists: C:\\Users\\maric\\stanza_resources\\en\\default.zip\n",
      "2024-12-09 19:05:43 INFO: Finished downloading models and saved to C:\\Users\\maric\\stanza_resources\n",
      "2024-12-09 19:05:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 33.5MB/s]                    \n",
      "2024-12-09 19:05:43 INFO: Downloaded file to C:\\Users\\maric\\stanza_resources\\resources.json\n",
      "2024-12-09 19:05:44 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-12-09 19:05:44 INFO: Using device: cpu\n",
      "2024-12-09 19:05:44 INFO: Loading: tokenize\n",
      "2024-12-09 19:05:44 INFO: Loading: mwt\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-09 19:05:44 INFO: Loading: pos\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-09 19:05:44 INFO: Loading: lemma\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-09 19:05:44 INFO: Loading: depparse\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-09 19:05:45 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from typing import List, Set\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from nltk.chunk import RegexpParser\n",
    "import copy\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse', tokenize_pretokenized=True)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# Download required resource\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Micron', 'has', 'declared', 'its', 'first', ...</td>\n",
       "      <td>['Micron', \"'s\", 'numbers', 'also', 'marked', ...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['The', 'fines', 'are', 'part', 'of', 'failed'...</td>\n",
       "      <td>['Perry', 'said', 'he', 'backs', 'the', 'Senat...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['The', 'tech-loaded', 'Nasdaq', 'composite', ...</td>\n",
       "      <td>['The', 'technology-laced', 'Nasdaq', 'Composi...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...   \n",
       "1  ['Micron', 'has', 'declared', 'its', 'first', ...   \n",
       "2  ['The', 'fines', 'are', 'part', 'of', 'failed'...   \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...   \n",
       "4  ['The', 'tech-loaded', 'Nasdaq', 'composite', ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...  4.00  \n",
       "1  ['Micron', \"'s\", 'numbers', 'also', 'marked', ...  3.75  \n",
       "2  ['Perry', 'said', 'he', 'backs', 'the', 'Senat...  2.80  \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...  3.40  \n",
       "4  ['The', 'technology-laced', 'Nasdaq', 'Composi...  2.40  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_token_df = pd.read_csv('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1   gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...  NaN  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  NaN  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...  NaN  \n",
       "3  [The, American, Anglican, Council, ,, which, r...  NaN  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...  NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the 2 first columns from strings to actual lists of strings\n",
    "\n",
    "train_df = pd.DataFrame(columns=['0','1','gs'], index=range(2234))\n",
    "train_df.iloc[:, :2] = train_token_df.iloc[:, :2].map(ast.literal_eval)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(But, CC), (other, JJ), (sources, NNS), (clos...</td>\n",
       "      <td>[(But, CC), (other, JJ), (sources, NNS), (clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Micron, NNP), (has, VBZ), (declared, VBN), (...</td>\n",
       "      <td>[(Micron, NNP), (s, NN), (numbers, NNS), (also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(The, DT), (fines, NNS), (are, VBP), (part, N...</td>\n",
       "      <td>[(Perry, NNP), (said, VBD), (he, PRP), (backs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(The, DT), (American, JJ), (Anglican, NNP), (...</td>\n",
       "      <td>[(The, DT), (American, JJ), (Anglican, NNP), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(The, DT), (tech-loaded, JJ), (Nasdaq, NNP), ...</td>\n",
       "      <td>[(The, DT), (technology-laced, JJ), (Nasdaq, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(But, CC), (other, JJ), (sources, NNS), (clos...   \n",
       "1  [(Micron, NNP), (has, VBZ), (declared, VBN), (...   \n",
       "2  [(The, DT), (fines, NNS), (are, VBP), (part, N...   \n",
       "3  [(The, DT), (American, JJ), (Anglican, NNP), (...   \n",
       "4  [(The, DT), (tech-loaded, JJ), (Nasdaq, NNP), ...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(But, CC), (other, JJ), (sources, NNS), (clos...  \n",
       "1  [(Micron, NNP), (s, NN), (numbers, NNS), (also...  \n",
       "2  [(Perry, NNP), (said, VBD), (he, PRP), (backs,...  \n",
       "3  [(The, DT), (American, JJ), (Anglican, NNP), (...  \n",
       "4  [(The, DT), (technology-laced, JJ), (Nasdaq, N...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Remove punctuation\n",
    "train_df = preprocessor.remove_punctuation(train_df)\n",
    "train_df = preprocessor.remove_empty_strings(train_df)\n",
    "\n",
    "# POS-tagging the words\n",
    "n=len(train_df)\n",
    "train_df_POS = pd.DataFrame(columns=['0','1'])\n",
    "\n",
    "for i in range(n):\n",
    "    train_df_POS.loc[i,'0'] = nltk.pos_tag(train_df.loc[i,'0']) \n",
    "    train_df_POS.loc[i,'1'] = nltk.pos_tag(train_df.loc[i,'1']) \n",
    "\n",
    "train_df_POS.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(other, JJ), (sources, NNS), (close, RB), (to...</td>\n",
       "      <td>[(other, JJ), (sources, NNS), (close, RB), (to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Micron, NNP), (has, VBZ), (declared, VBN), (...</td>\n",
       "      <td>[(Micron, NNP), (s, NN), (numbers, NNS), (also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(fines, NNS), (are, VBP), (part, NN), (failed...</td>\n",
       "      <td>[(Perry, NNP), (said, VBD), (he, PRP), (backs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(American, JJ), (Anglican, NNP), (Council, NN...</td>\n",
       "      <td>[(American, JJ), (Anglican, NNP), (Council, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(tech-loaded, JJ), (Nasdaq, NNP), (composite,...</td>\n",
       "      <td>[(technology-laced, JJ), (Nasdaq, NNP), (Compo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(other, JJ), (sources, NNS), (close, RB), (to...   \n",
       "1  [(Micron, NNP), (has, VBZ), (declared, VBN), (...   \n",
       "2  [(fines, NNS), (are, VBP), (part, NN), (failed...   \n",
       "3  [(American, JJ), (Anglican, NNP), (Council, NN...   \n",
       "4  [(tech-loaded, JJ), (Nasdaq, NNP), (composite,...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(other, JJ), (sources, NNS), (close, RB), (to...  \n",
       "1  [(Micron, NNP), (s, NN), (numbers, NNS), (also...  \n",
       "2  [(Perry, NNP), (said, VBD), (he, PRP), (backs,...  \n",
       "3  [(American, JJ), (Anglican, NNP), (Council, NN...  \n",
       "4  [(technology-laced, JJ), (Nasdaq, NNP), (Compo...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function words (prepositions, conjunctions, articles) carry less semantics than content words \n",
    "# and thus removing them might eliminate the noise and provide a more accurate estimate of semantic similarity.\n",
    "\n",
    "function_words_tag = {'IN', 'CC', 'DT', 'PDT', 'WDT'}\n",
    "\n",
    "# Create a deep copy of the DataFrame\n",
    "train_df_POS_bis = copy.deepcopy(train_df_POS)\n",
    "\n",
    "# Iterate through the rows and modify columns '0' and '1'\n",
    "for i in range(n):\n",
    "    for tag in function_words_tag:\n",
    "        # Extract, modify, and reassign the list in column '0'\n",
    "        col_0 = train_df_POS_bis.at[i, '0']\n",
    "        train_df_POS_bis.at[i, '0'] = [item for item in col_0 if item[1] != tag]\n",
    "\n",
    "        # Extract, modify, and reassign the list in column '1'\n",
    "        col_1 = train_df_POS_bis.at[i, '1']\n",
    "        train_df_POS_bis.at[i, '1'] = [item for item in col_1 if item[1] != tag]\n",
    "\n",
    "train_df_POS_bis.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[other, sources, close, to, sale, said, Vivend...</td>\n",
       "      <td>[other, sources, close, to, sale, said, Vivend...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, s, numbers, also, marked, first, quar...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fines, are, part, failed, Republican, efforts...</td>\n",
       "      <td>[Perry, said, he, backs, Senate, s, efforts, i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[American, Anglican, Council, represents, Epis...</td>\n",
       "      <td>[American, Anglican, Council, represents, Epis...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, Nasdaq, composite, rose, 20.96, ...</td>\n",
       "      <td>[technology-laced, Nasdaq, Composite, Index, I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Amgen, shares, gained, 93, cents, 1.45, perce...</td>\n",
       "      <td>[Shares, Allergan, were, up, 14, cents, 78.40,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[U.S, prosecutors, have, arrested, more, 130, ...</td>\n",
       "      <td>[More, 130, people, have, been, arrested, 17, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Chavez, said, investigators, feel, confident,...</td>\n",
       "      <td>[Albuquerque, Mayor, Martin, Chavez, said, inv...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Authorities, said, scientist, properly, quara...</td>\n",
       "      <td>[scientist, also, quarantined, himself, home, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[support, will, come, free, software, upgrade,...</td>\n",
       "      <td>[upgrade, will, be, available, free, download,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [other, sources, close, to, sale, said, Vivend...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [fines, are, part, failed, Republican, efforts...   \n",
       "3  [American, Anglican, Council, represents, Epis...   \n",
       "4  [tech-loaded, Nasdaq, composite, rose, 20.96, ...   \n",
       "5  [Amgen, shares, gained, 93, cents, 1.45, perce...   \n",
       "6  [U.S, prosecutors, have, arrested, more, 130, ...   \n",
       "7  [Chavez, said, investigators, feel, confident,...   \n",
       "8  [Authorities, said, scientist, properly, quara...   \n",
       "9  [support, will, come, free, software, upgrade,...   \n",
       "\n",
       "                                                   1   gs  \n",
       "0  [other, sources, close, to, sale, said, Vivend...  NaN  \n",
       "1  [Micron, s, numbers, also, marked, first, quar...  NaN  \n",
       "2  [Perry, said, he, backs, Senate, s, efforts, i...  NaN  \n",
       "3  [American, Anglican, Council, represents, Epis...  NaN  \n",
       "4  [technology-laced, Nasdaq, Composite, Index, I...  NaN  \n",
       "5  [Shares, Allergan, were, up, 14, cents, 78.40,...  NaN  \n",
       "6  [More, 130, people, have, been, arrested, 17, ...  NaN  \n",
       "7  [Albuquerque, Mayor, Martin, Chavez, said, inv...  NaN  \n",
       "8  [scientist, also, quarantined, himself, home, ...  NaN  \n",
       "9  [upgrade, will, be, available, free, download,...  NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1 = pd.DataFrame(columns=['0','1','gs'])\n",
    "\n",
    "for i in range(n):\n",
    "    sentence=[]\n",
    "    for j in range(len(train_df_POS_bis.loc[i,'0'])):\n",
    "        sentence.append(train_df_POS_bis.loc[i,'0'][j][0])\n",
    "    train_df_1.loc[i,'0']=sentence\n",
    "    sentence=[]\n",
    "    for k in range(len(train_df_POS_bis.loc[i,'1'])):\n",
    "        sentence.append(train_df_POS_bis.loc[i,'1'][k][0])\n",
    "    train_df_1.loc[i,'1']=sentence\n",
    "\n",
    "train_df_1['gs'] = train_df['gs']\n",
    "train_df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_overlap(tokens1: List[str], tokens2: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the n-gram overlap between two tokenized sentences.\n",
    "\n",
    "    Parameters:\n",
    "        tokens1 (List[str]): Tokenized first sentence as a list of strings.\n",
    "        tokens2 (List[str]): Tokenized second sentence as a list of strings.\n",
    "        n (int): The size of n-grams.\n",
    "\n",
    "    Returns:\n",
    "        float: The n-gram overlap ratio.\n",
    "    \"\"\"\n",
    "    def generate_ngrams(tokens: List[str], n: int) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Generates n-grams for a given list of tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        return set([' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
    "\n",
    "    # Generate n-grams for both token lists\n",
    "    ngrams_s1 = generate_ngrams(tokens1, n)\n",
    "    ngrams_s2 = generate_ngrams(tokens2, n)\n",
    "\n",
    "    # Compute the intersection \n",
    "    intersection = ngrams_s1.intersection(ngrams_s2)\n",
    "\n",
    "    # Compute the n gram overlap when posible\n",
    "    if len(intersection)==0:\n",
    "        ngo=0\n",
    "    else:\n",
    "        ngo=2/((len(ngrams_s1)+len(ngrams_s2))/len(intersection))\n",
    "\n",
    "    return float(ngo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_tagging_unigrams</th>\n",
       "      <th>POS_tagging_bigrams</th>\n",
       "      <th>POS_tagging_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POS_tagging_unigrams  POS_tagging_bigrams  POS_tagging_trigrams\n",
       "0              0.702703             0.594595              0.514286\n",
       "1              0.571429             0.421053              0.352941\n",
       "2              0.500000             0.250000              0.090909\n",
       "3              0.777778             0.764706              0.750000\n",
       "4              0.230769             0.000000              0.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_features=pd.DataFrame(columns=['POS_tagging_unigrams','POS_tagging_bigrams','POS_tagging_trigrams'])\n",
    "\n",
    "for i in range(n):\n",
    "    # unigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_unigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],1)\n",
    "    # bigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_bigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],2)\n",
    "    # trigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_trigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],3)\n",
    "\n",
    "\n",
    "# Convert all columns in a DataFrame to numeric, coercing errors into NaN.\n",
    "syntactic_features['POS_tagging_unigrams'] = pd.to_numeric(syntactic_features['POS_tagging_unigrams'], errors='coerce') \n",
    "syntactic_features['POS_tagging_bigrams'] = pd.to_numeric(syntactic_features['POS_tagging_bigrams'], errors='coerce') \n",
    "syntactic_features['POS_tagging_trigrams'] = pd.to_numeric(syntactic_features['POS_tagging_trigrams'], errors='coerce') \n",
    "\n",
    "syntactic_features.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFY WORDS IN SET OF 's','p' or 'o'\n",
    "\n",
    "def classify_syntactic_roles(doc):\n",
    "    roles = {'p': [], 's': [], 'o': []}  # Initialize lists for multiple sets of p, s, o   \n",
    "    for sentence in doc.sentences:        \n",
    "        # Initialize temporary sets for predicates, subjects, and objects\n",
    "        temp_p = set()  # predicates\n",
    "        subject_sets = []  # List of sets for subjects\n",
    "        object_sets = []  # List of sets for objects\n",
    "        \n",
    "        prep_objs = {}  # To store prepositional objects linked to the same predicate\n",
    "        \n",
    "        # Process each word in the sentence\n",
    "        for word in sentence.words:\n",
    "            if word.deprel == 'root':  # The predicate (main verb)\n",
    "                temp_p.add(word.text)\n",
    "            elif word.deprel in ['nsubj', 'nsubjpass']:  # Subject\n",
    "                # Check if it belongs to an existing set\n",
    "                head_text = sentence.words[word.head - 1].text if word.head > 0 else None\n",
    "                added = False\n",
    "                for s_set in subject_sets:\n",
    "                    if any(head_text == sentence.words[s.head - 1].text for s in sentence.words if s.text in s_set):\n",
    "                        s_set.add(word.text)\n",
    "                        added = True\n",
    "                        break\n",
    "                if not added:\n",
    "                    subject_sets.append({word.text})  # Create a new set\n",
    "            elif word.deprel in ['obj', 'dobj', 'obl', 'case']:  # Object\n",
    "                object_sets.append({word.text})  # Create a set for this object\n",
    "            elif word.deprel == 'conj':  # Conjunction linking words\n",
    "                head = sentence.words[word.head - 1].text  # The word it is conjoined with\n",
    "                # If head is a subject, add the conjunct word to the subject set\n",
    "                for s_set in subject_sets:\n",
    "                    if head in s_set:\n",
    "                        s_set.add(word.text)\n",
    "                        break\n",
    "                # If head is an object, add the conjunct word to the object set\n",
    "                for obj_set in object_sets:\n",
    "                    if head in obj_set:\n",
    "                        obj_set.add(word.text)\n",
    "                        break\n",
    "                # If head is a predicate, add the conjunct word to the predicate set\n",
    "                if head in temp_p: # coordinated structure\n",
    "                    temp_p.add(word.text)\n",
    "            elif word.deprel == 'ccomp': # subordinate clausures\n",
    "                head = sentence.words[word.head - 1].text  # The word it is conjoined with\n",
    "                if head in temp_p:\n",
    "                    temp_p.add(word.text)\n",
    "\n",
    "        # EXTRA: Merge object sets based on the specified rules\n",
    "        merged_object_sets = []\n",
    "        while object_sets:\n",
    "            current_set = object_sets.pop(0)\n",
    "            merged = False\n",
    "            for other_set in object_sets:\n",
    "                # Rule 1: Check if any 'case' word's head is in another set\n",
    "                for word in current_set:\n",
    "                    for other_word in sentence.words:\n",
    "                        if other_word.text == word and other_word.deprel == 'case':\n",
    "                            head_word = sentence.words[other_word.head - 1] if other_word.head > 0 else None\n",
    "                            if head_word and any(head_word.text in s for s in object_sets):\n",
    "                                other_set.update(current_set)\n",
    "                                merged = True\n",
    "                                break\n",
    "                    if merged:\n",
    "                        break\n",
    "                if merged:\n",
    "                    break\n",
    "                # Check if two words in different sets share the same head\n",
    "                for word1 in current_set:\n",
    "                    for word2 in other_set:\n",
    "                        word1_head = next((w.head for w in sentence.words if w.text == word1), None)\n",
    "                        word2_head = next((w.head for w in sentence.words if w.text == word2), None)\n",
    "                        if word1_head and word2_head and word1_head == word2_head:\n",
    "                            other_set.update(current_set)\n",
    "                            merged = True\n",
    "                            break\n",
    "                if merged:\n",
    "                    break\n",
    "            if not merged:\n",
    "                merged_object_sets.append(current_set)\n",
    "\n",
    "        # Assign to the roles dictionary\n",
    "        for predicate in temp_p:\n",
    "            roles['p'].append({predicate})  # Create a set for each predicate\n",
    "        \n",
    "        for s_set in subject_sets:\n",
    "            roles['s'].append(s_set)  # Group subjects as their respective sets\n",
    "        \n",
    "        for obj_set in merged_object_sets:\n",
    "            roles['o'].append(obj_set)  # Group objects as their respective sets\n",
    "\n",
    "            # Replace words in the sets with their lemmas\n",
    "        for role in roles:\n",
    "            for idx, word_set in enumerate(roles[role]):\n",
    "                roles[role][idx] = {sentence.words[next(i for i, w in enumerate(sentence.words) if w.text == word)].lemma \n",
    "                                    for word in word_set}\n",
    "\n",
    "        return roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordnet tags\n",
    "d = {'NN': 'n', 'NNS': 'n',\n",
    "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
    "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
    "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
    "\n",
    "\n",
    "# function to obtain a synset from a word.\n",
    "def extract_synset(w):\n",
    "  pair=nltk.pos_tag([w])\n",
    "  if pair[0][1] in d.keys(): # Check if has a wordnet tag\n",
    "    word_synsets = wn.synsets(w,d[pair[0][1]])\n",
    "    return word_synsets[0]\n",
    "  \n",
    "  else:\n",
    "    print('The word ',w,' has no wordnet tag.')\n",
    "    return False\n",
    "\n",
    "\n",
    "def chunksim(c1,c2):\n",
    "    # compute lin similarity first\n",
    "    sim_score=0\n",
    "    for l1 in c1:\n",
    "        for l2 in c2:\n",
    "            synset_l1=extract_synset(l1)\n",
    "            synset_l2=extract_synset(l2)\n",
    "            if synset_l1.pos()==synset_l2.pos():\n",
    "\n",
    "                # Calculate Lin Similarity\n",
    "                sim_score += synset_l1.lin_similarity(synset_l2, brown_ic)\n",
    "    if sim_score==0:\n",
    "        return 0\n",
    "    else:\n",
    "        ckn1=sim_score/len(c1)\n",
    "        ckn2=sim_score/len(c2)\n",
    "\n",
    "        return 2*ckn1*ckn2/(ckn1+ckn2) #harmonic mean \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks\n",
    "\n",
    "chunk_sim_df = pd.DataFrame(columns=['Chunk_Sim'], index=range(n))\n",
    "\n",
    "for k in range(1):\n",
    "    # Classify the words into p, s, o for each sentence \n",
    "    roles_0 = classify_syntactic_roles(nlp_stanza([train_df.loc[k,'0']]))\n",
    "    roles_1 = classify_syntactic_roles(nlp_stanza([train_df.loc[k,'1']]))\n",
    "    # compute the simchunk for each pair of sentence and save only the sim value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'say'}, {'hope'}, {'keep'}], [{'say'}, {'keep'}])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roles_0['p'], roles_1['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates_0= roles_0['p']\n",
    "predicates_1=roles_1['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len0=len(predicates_0)\n",
    "len1=len(predicates_1)\n",
    "\n",
    "linsim_ = np.zeros((len0,len1))\n",
    "\n",
    "for i in range(len0):\n",
    "    for j in range(len1):\n",
    "        linsim_[i,j] = chunksim(predicates_0[i],predicates_1[j])\n",
    "\n",
    "if len0 > len1:\n",
    "    max_val=np.zeros(len1)\n",
    "    # si el numero de filas es mayor que el numero de columnas, tomo el maximo de cada columna\n",
    "    for j in range(len1):\n",
    "        # Encuentra el valor m치ximo en la columna j\n",
    "        max_val[j] = np.max(linsim_[:, :])\n",
    "        print(np.argmax(linsim_[:, :])) # no devuele los dos indices solo uno mirar q esta haciendo esto\n",
    "        # la columna donde he encontrado el primer m치ximo se vuelve cero\n",
    "        #linsim_[:,np.argmax(linsim_[:, :])[1]]=0\n",
    "    # lleno max_val con los valores maximos\n",
    "    sim_p=np.mean(max_val)\n",
    "    \n",
    "else:\n",
    "    #tomo el maximo de cada fila\n",
    "    max_val=np.zeros(len0)\n",
    "    # si el numero de filas es mayor que el numero de columnas, tomo el maximo de cada columna\n",
    "    for i in range(len0):\n",
    "        # Encuentra el valor m치ximo en la columna j\n",
    "        max_val[i] = np.max(linsim_[:, :])\n",
    "\n",
    "        # la fila donde he encontrado el primer m치ximo se vuelve cero\n",
    "        linsim_[np.argmax(linsim_[:, :])[0],:]=0\n",
    "        \n",
    "    # lleno max_val con los valores maximos\n",
    "    sim_p=np.mean(max_val)\n",
    "\n",
    "max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IHLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
