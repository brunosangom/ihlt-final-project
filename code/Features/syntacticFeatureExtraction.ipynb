{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from typing import List, Set\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from nltk.chunk import RegexpParser\n",
    "import copy\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# Download required resource\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Micron', 'has', 'declared', 'its', 'first', ...</td>\n",
       "      <td>['Micron', \"'s\", 'numbers', 'also', 'marked', ...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['The', 'fines', 'are', 'part', 'of', 'failed'...</td>\n",
       "      <td>['Perry', 'said', 'he', 'backs', 'the', 'Senat...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['The', 'tech-loaded', 'Nasdaq', 'composite', ...</td>\n",
       "      <td>['The', 'technology-laced', 'Nasdaq', 'Composi...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...   \n",
       "1  ['Micron', 'has', 'declared', 'its', 'first', ...   \n",
       "2  ['The', 'fines', 'are', 'part', 'of', 'failed'...   \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...   \n",
       "4  ['The', 'tech-loaded', 'Nasdaq', 'composite', ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...  4.00  \n",
       "1  ['Micron', \"'s\", 'numbers', 'also', 'marked', ...  3.75  \n",
       "2  ['Perry', 'said', 'he', 'backs', 'the', 'Senat...  2.80  \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...  3.40  \n",
       "4  ['The', 'technology-laced', 'Nasdaq', 'Composi...  2.40  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_df = pd.read_csv('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...  4.00  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  3.75  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...  2.80  \n",
       "3  [The, American, Anglican, Council, ,, which, r...  3.40  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...  2.40  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the 2 first columns from strings to actual lists of strings\n",
    "train_df.iloc[:, :2] = train_df.iloc[:, :2].map(ast.literal_eval)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(But, CC), (other, JJ), (sources, NNS), (clos...</td>\n",
       "      <td>[(But, CC), (other, JJ), (sources, NNS), (clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Micron, NNP), (has, VBZ), (declared, VBN), (...</td>\n",
       "      <td>[(Micron, NNP), (s, NN), (numbers, NNS), (also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(The, DT), (fines, NNS), (are, VBP), (part, N...</td>\n",
       "      <td>[(Perry, NNP), (said, VBD), (he, PRP), (backs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(The, DT), (American, JJ), (Anglican, NNP), (...</td>\n",
       "      <td>[(The, DT), (American, JJ), (Anglican, NNP), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(The, DT), (tech-loaded, JJ), (Nasdaq, NNP), ...</td>\n",
       "      <td>[(The, DT), (technology-laced, JJ), (Nasdaq, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(But, CC), (other, JJ), (sources, NNS), (clos...   \n",
       "1  [(Micron, NNP), (has, VBZ), (declared, VBN), (...   \n",
       "2  [(The, DT), (fines, NNS), (are, VBP), (part, N...   \n",
       "3  [(The, DT), (American, JJ), (Anglican, NNP), (...   \n",
       "4  [(The, DT), (tech-loaded, JJ), (Nasdaq, NNP), ...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(But, CC), (other, JJ), (sources, NNS), (clos...  \n",
       "1  [(Micron, NNP), (s, NN), (numbers, NNS), (also...  \n",
       "2  [(Perry, NNP), (said, VBD), (he, PRP), (backs,...  \n",
       "3  [(The, DT), (American, JJ), (Anglican, NNP), (...  \n",
       "4  [(The, DT), (technology-laced, JJ), (Nasdaq, N...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Remove punctuation\n",
    "train_df = preprocessor.remove_punctuation(train_df)\n",
    "train_df = preprocessor.remove_empty_strings(train_df)\n",
    "\n",
    "# POS-tagging the words\n",
    "\n",
    "n=len(train_df)\n",
    "train_df_POS = pd.DataFrame(columns=['0','1'])\n",
    "\n",
    "for i in range(n):\n",
    "    train_df_POS.loc[i,'0'] = nltk.pos_tag(train_df.loc[i,'0']) \n",
    "    train_df_POS.loc[i,'1'] = nltk.pos_tag(train_df.loc[i,'1']) \n",
    "\n",
    "train_df_POS.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(other, JJ), (sources, NNS), (close, RB), (to...</td>\n",
       "      <td>[(other, JJ), (sources, NNS), (close, RB), (to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Micron, NNP), (has, VBZ), (declared, VBN), (...</td>\n",
       "      <td>[(Micron, NNP), (s, NN), (numbers, NNS), (also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(fines, NNS), (are, VBP), (part, NN), (failed...</td>\n",
       "      <td>[(Perry, NNP), (said, VBD), (he, PRP), (backs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(American, JJ), (Anglican, NNP), (Council, NN...</td>\n",
       "      <td>[(American, JJ), (Anglican, NNP), (Council, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(tech-loaded, JJ), (Nasdaq, NNP), (composite,...</td>\n",
       "      <td>[(technology-laced, JJ), (Nasdaq, NNP), (Compo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(other, JJ), (sources, NNS), (close, RB), (to...   \n",
       "1  [(Micron, NNP), (has, VBZ), (declared, VBN), (...   \n",
       "2  [(fines, NNS), (are, VBP), (part, NN), (failed...   \n",
       "3  [(American, JJ), (Anglican, NNP), (Council, NN...   \n",
       "4  [(tech-loaded, JJ), (Nasdaq, NNP), (composite,...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(other, JJ), (sources, NNS), (close, RB), (to...  \n",
       "1  [(Micron, NNP), (s, NN), (numbers, NNS), (also...  \n",
       "2  [(Perry, NNP), (said, VBD), (he, PRP), (backs,...  \n",
       "3  [(American, JJ), (Anglican, NNP), (Council, NN...  \n",
       "4  [(technology-laced, JJ), (Nasdaq, NNP), (Compo...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function words (prepositions, conjunctions, articles) carry less semantics than content words \n",
    "# and thus removing them might eliminate the noise and provide a more accurate estimate of semantic similarity.\n",
    "\n",
    "function_words_tag = {'IN', 'CC', 'DT', 'PDT', 'WDT'}\n",
    "\n",
    "# Create a deep copy of the DataFrame\n",
    "train_df_POS_bis = copy.deepcopy(train_df_POS)\n",
    "\n",
    "# Iterate through the rows and modify columns '0' and '1'\n",
    "for i in range(n):\n",
    "    for tag in function_words_tag:\n",
    "        # Extract, modify, and reassign the list in column '0'\n",
    "        col_0 = train_df_POS_bis.at[i, '0']\n",
    "        train_df_POS_bis.at[i, '0'] = [item for item in col_0 if item[1] != tag]\n",
    "\n",
    "        # Extract, modify, and reassign the list in column '1'\n",
    "        col_1 = train_df_POS_bis.at[i, '1']\n",
    "        train_df_POS_bis.at[i, '1'] = [item for item in col_1 if item[1] != tag]\n",
    "\n",
    "train_df_POS_bis.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[other, sources, close, to, sale, said, Vivend...</td>\n",
       "      <td>[other, sources, close, to, sale, said, Vivend...</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, s, numbers, also, marked, first, quar...</td>\n",
       "      <td>3.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fines, are, part, failed, Republican, efforts...</td>\n",
       "      <td>[Perry, said, he, backs, Senate, s, efforts, i...</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[American, Anglican, Council, represents, Epis...</td>\n",
       "      <td>[American, Anglican, Council, represents, Epis...</td>\n",
       "      <td>3.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, Nasdaq, composite, rose, 20.96, ...</td>\n",
       "      <td>[technology-laced, Nasdaq, Composite, Index, I...</td>\n",
       "      <td>2.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Amgen, shares, gained, 93, cents, 1.45, perce...</td>\n",
       "      <td>[Shares, Allergan, were, up, 14, cents, 78.40,...</td>\n",
       "      <td>1.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[U.S, prosecutors, have, arrested, more, 130, ...</td>\n",
       "      <td>[More, 130, people, have, been, arrested, 17, ...</td>\n",
       "      <td>4.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Chavez, said, investigators, feel, confident,...</td>\n",
       "      <td>[Albuquerque, Mayor, Martin, Chavez, said, inv...</td>\n",
       "      <td>3.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Authorities, said, scientist, properly, quara...</td>\n",
       "      <td>[scientist, also, quarantined, himself, home, ...</td>\n",
       "      <td>4.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[support, will, come, free, software, upgrade,...</td>\n",
       "      <td>[upgrade, will, be, available, free, download,...</td>\n",
       "      <td>2.600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [other, sources, close, to, sale, said, Vivend...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [fines, are, part, failed, Republican, efforts...   \n",
       "3  [American, Anglican, Council, represents, Epis...   \n",
       "4  [tech-loaded, Nasdaq, composite, rose, 20.96, ...   \n",
       "5  [Amgen, shares, gained, 93, cents, 1.45, perce...   \n",
       "6  [U.S, prosecutors, have, arrested, more, 130, ...   \n",
       "7  [Chavez, said, investigators, feel, confident,...   \n",
       "8  [Authorities, said, scientist, properly, quara...   \n",
       "9  [support, will, come, free, software, upgrade,...   \n",
       "\n",
       "                                                   1     gs  \n",
       "0  [other, sources, close, to, sale, said, Vivend...  4.000  \n",
       "1  [Micron, s, numbers, also, marked, first, quar...  3.750  \n",
       "2  [Perry, said, he, backs, Senate, s, efforts, i...  2.800  \n",
       "3  [American, Anglican, Council, represents, Epis...  3.400  \n",
       "4  [technology-laced, Nasdaq, Composite, Index, I...  2.400  \n",
       "5  [Shares, Allergan, were, up, 14, cents, 78.40,...  1.333  \n",
       "6  [More, 130, people, have, been, arrested, 17, ...  4.600  \n",
       "7  [Albuquerque, Mayor, Martin, Chavez, said, inv...  3.800  \n",
       "8  [scientist, also, quarantined, himself, home, ...  4.200  \n",
       "9  [upgrade, will, be, available, free, download,...  2.600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1 = pd.DataFrame(columns=['0','1','gs'])\n",
    "\n",
    "for i in range(n):\n",
    "    sentence=[]\n",
    "    for j in range(len(train_df_POS_bis.loc[i,'0'])):\n",
    "        sentence.append(train_df_POS_bis.loc[i,'0'][j][0])\n",
    "    train_df_1.loc[i,'0']=sentence\n",
    "    sentence=[]\n",
    "    for k in range(len(train_df_POS_bis.loc[i,'1'])):\n",
    "        sentence.append(train_df_POS_bis.loc[i,'1'][k][0])\n",
    "    train_df_1.loc[i,'1']=sentence\n",
    "\n",
    "train_df_1['gs'] = train_df['gs']\n",
    "train_df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_overlap(tokens1: List[str], tokens2: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the n-gram overlap between two tokenized sentences.\n",
    "\n",
    "    Parameters:\n",
    "        tokens1 (List[str]): Tokenized first sentence as a list of strings.\n",
    "        tokens2 (List[str]): Tokenized second sentence as a list of strings.\n",
    "        n (int): The size of n-grams.\n",
    "\n",
    "    Returns:\n",
    "        float: The n-gram overlap ratio.\n",
    "    \"\"\"\n",
    "    def generate_ngrams(tokens: List[str], n: int) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Generates n-grams for a given list of tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        return set([' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
    "\n",
    "    # Generate n-grams for both token lists\n",
    "    ngrams_s1 = generate_ngrams(tokens1, n)\n",
    "    ngrams_s2 = generate_ngrams(tokens2, n)\n",
    "\n",
    "    # Compute the intersection \n",
    "    intersection = ngrams_s1.intersection(ngrams_s2)\n",
    "\n",
    "    # Compute the n gram overlap when posible\n",
    "    if len(intersection)==0:\n",
    "        ngo=0\n",
    "    else:\n",
    "        ngo=2/((len(ngrams_s1)+len(ngrams_s2))/len(intersection))\n",
    "\n",
    "    return float(ngo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_tagging_unigrams</th>\n",
       "      <th>POS_tagging_bigrams</th>\n",
       "      <th>POS_tagging_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POS_tagging_unigrams  POS_tagging_bigrams  POS_tagging_trigrams\n",
       "0              0.702703             0.594595              0.514286\n",
       "1              0.571429             0.421053              0.352941\n",
       "2              0.500000             0.250000              0.090909\n",
       "3              0.777778             0.764706              0.750000\n",
       "4              0.230769             0.000000              0.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_features=pd.DataFrame(columns=['POS_tagging_unigrams','POS_tagging_bigrams','POS_tagging_trigrams'])\n",
    "\n",
    "for i in range(n):\n",
    "    # unigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_unigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],1)\n",
    "    # bigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_bigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],2)\n",
    "    # trigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_trigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],3)\n",
    "\n",
    "\n",
    "# Convert all columns in a DataFrame to numeric, coercing errors into NaN.\n",
    "syntactic_features['POS_tagging_unigrams'] = pd.to_numeric(syntactic_features['POS_tagging_unigrams'], errors='coerce') \n",
    "syntactic_features['POS_tagging_bigrams'] = pd.to_numeric(syntactic_features['POS_tagging_bigrams'], errors='coerce') \n",
    "syntactic_features['POS_tagging_trigrams'] = pd.to_numeric(syntactic_features['POS_tagging_trigrams'], errors='coerce') \n",
    "\n",
    "syntactic_features.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_chunks(pos_tagged):\n",
    "    \"\"\"\n",
    "    Extract chunks from a tokenized sentence using NLTK.\n",
    "    \n",
    "    \"\"\"\n",
    "    grammar = r\"\"\"\n",
    "    # Verb phrase components\n",
    "    p: \n",
    "        {<VBD><VBG>}              # Past progressive (e.g., \"was eating\")\n",
    "        {<VBZ><VBG>}                    # Progressive form (e.g., \"is eating\")\n",
    "        {<VBZ><VBN>}                    # Passive form (e.g., \"is eaten\")\n",
    "        {<VBZ><JJ>}                     # Copular construction (e.g., \"is happy\")\n",
    "        {<VBN>}                   # Perfect construction (e.g., \"has driven\")\n",
    "        {<VBD><VBN>}              # Past perfect (e.g., \"had driven\")\n",
    "        {<MD>?<VB.*><RB>*}              # Modal + verb + optional adverb\n",
    "        {<VB.*><RP>?}                   # Verb with optional particle\n",
    "\n",
    "    # Subject (typically occurs before VP)\n",
    "    # Noun phrase components\n",
    "    s:\n",
    "        {<DT>?<JJ.*>*<NN.*>+}           # Basic noun phrase\n",
    "        {<PRP>}                         # Pronouns\n",
    "        {<NNP>+}                        # Proper nouns\n",
    "        }<p>{\n",
    "        \n",
    "\n",
    "    # Object (must follow VP)\n",
    "    o:\n",
    "        }<p>{  \n",
    "        {<DT>?<JJ.*>*<NN.*>+}           # Basic noun phrase\n",
    "        {<PRP>}                         # Pronouns\n",
    "        {<NNP>+}                        # Proper nouns\n",
    "        {<IN><s>}                      # Prepositional object\n",
    "        {<TO><s>}                      # 'To' prepositional phrase\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a chunk parser with our grammar\n",
    "    chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "    # Perform chunking\n",
    "    chunked = chunk_parser.parse(pos_tagged)\n",
    "    \n",
    "    # Extract chunks into a more readable format\n",
    "    chunks = []\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() != 'S'):\n",
    "        words = [word for word, tag in subtree.leaves()]\n",
    "        chunks.append((subtree.label(), words))\n",
    "        #chunks.append(words)\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(s, [other, sources]), (o, [to, the, sale]), ...</td>\n",
       "      <td>[(s, [other, sources]), (o, [to, the, sale]), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(s, [Micron]), (p, [has, declared]), (s, [fir...</td>\n",
       "      <td>[(s, [Micron, s, numbers]), (p, [marked]), (s,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(s, [The, fines]), (p, [are]), (s, [part]), (...</td>\n",
       "      <td>[(s, [Perry]), (p, [said]), (s, [he]), (p, [ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(s, [The, American, Anglican, Council]), (p, ...</td>\n",
       "      <td>[(s, [The, American, Anglican, Council]), (p, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(s, [The, tech-loaded, Nasdaq, composite]), (...</td>\n",
       "      <td>[(s, [The, technology-laced, Nasdaq, Composite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(s, [other, sources]), (o, [to, the, sale]), ...   \n",
       "1  [(s, [Micron]), (p, [has, declared]), (s, [fir...   \n",
       "2  [(s, [The, fines]), (p, [are]), (s, [part]), (...   \n",
       "3  [(s, [The, American, Anglican, Council]), (p, ...   \n",
       "4  [(s, [The, tech-loaded, Nasdaq, composite]), (...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(s, [other, sources]), (o, [to, the, sale]), ...  \n",
       "1  [(s, [Micron, s, numbers]), (p, [marked]), (s,...  \n",
       "2  [(s, [Perry]), (p, [said]), (s, [he]), (p, [ba...  \n",
       "3  [(s, [The, American, Anglican, Council]), (p, ...  \n",
       "4  [(s, [The, technology-laced, Nasdaq, Composite...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the sets of chunks of the first and the second sentence\n",
    "\n",
    "train_df_chunks = pd.DataFrame(columns=['0','1'])\n",
    "\n",
    "for i in range(n):\n",
    "    train_df_chunks.loc[i,'0'] = get_sentence_chunks(train_df_POS.loc[i,'0'])\n",
    "    train_df_chunks.loc[i,'1'] = get_sentence_chunks(train_df_POS.loc[i,'1'])\n",
    "\n",
    "train_df_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordnet tags\n",
    "d = {'NN': 'n', 'NNS': 'n',\n",
    "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
    "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
    "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
    "\n",
    "\n",
    "# function to obtain a synset from a word.\n",
    "def extract_synset(w):\n",
    "  pair=nltk.pos_tag([w])\n",
    "  if pair[0][1] in d.keys(): # Check if has a wordnet tag\n",
    "    word_synsets = wn.synsets(w,d[pair[0][1]])\n",
    "    return word_synsets[0]\n",
    "  \n",
    "  else:\n",
    "    print('The word ',w,' has no wordnet tag.')\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunksim(c1,c2):\n",
    "    # compute lin similarity first\n",
    "    sim_score=0\n",
    "    for l1 in c1:\n",
    "        for l2 in c2:\n",
    "            synset_l1=extract_synset(l1)\n",
    "            synset_l2=extract_synset(l2)\n",
    "            print(synset_l1,synset_l2)\n",
    "            if synset_l1.pos()==synset_l2.pos():\n",
    "\n",
    "                # Calculate Lin Similarity\n",
    "                sim_score += synset_l1.lin_similarity(synset_l2, brown_ic)\n",
    "\n",
    "    return sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('state.v.01') Synset('state.v.01')\n",
      "Synset('state.v.01') Synset('be.v.01')\n",
      "Synset('state.v.01') Synset('keep.v.01')\n",
      "Synset('be.v.01') Synset('state.v.01')\n",
      "Synset('keep.v.01') Synset('state.v.01')\n",
      "Synset('be.v.01') Synset('be.v.01')\n",
      "Synset('be.v.01') Synset('keep.v.01')\n",
      "Synset('keep.v.01') Synset('be.v.01')\n",
      "Synset('keep.v.01') Synset('keep.v.01')\n",
      "Synset('hope.v.01') Synset('state.v.01')\n",
      "Synset('hope.v.01') Synset('be.v.01')\n",
      "Synset('hope.v.01') Synset('keep.v.01')\n",
      "Synset('see.v.01') Synset('state.v.01')\n",
      "Synset('see.v.01') Synset('be.v.01')\n",
      "Synset('see.v.01') Synset('keep.v.01')\n",
      "Synset('team.n.01') Synset('state.v.01')\n",
      "Synset('team.n.01') Synset('be.v.01')\n",
      "Synset('team.n.01') Synset('keep.v.01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 2.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_0=[]\n",
    "\n",
    "for tupla in train_df_chunks.loc[0,'0']:\n",
    "    if tupla[0]=='p':\n",
    "        predicates_0.append(tupla[1])\n",
    "\n",
    "\n",
    "predicates_1=[]\n",
    "\n",
    "for tupla in train_df_chunks.loc[0,'1']:\n",
    "    if tupla[0]=='p':\n",
    "        predicates_1.append(tupla[1])\n",
    "\n",
    "len0=len(predicates_0)\n",
    "len1=len(predicates_1)\n",
    "\n",
    "linsim_ = np.zeros((len0,len1))\n",
    "for i in range(len0):\n",
    "    for j in range(len1):\n",
    "        linsim_[i,j] = chunksim(predicates_0[i],predicates_1[j])\n",
    "\n",
    "if len0 > len1:\n",
    "    max_val=np.zeros(len1)\n",
    "    # si el numero de filas es mayor que el numero de columnas, tomo el maximo de cada columna\n",
    "    for j in range():\n",
    "        # Encuentra el valor m치ximo en la columna j\n",
    "        max_val[j] = np.max(linsim_[:, j])\n",
    "\n",
    "\n",
    "        # Encuentra la fila donde ocurre el valor m치ximo\n",
    "        i = np.argmax(linsim_[:, j])\n",
    "else:\n",
    "    #tomo el maximo de cada fila\n",
    "    max_val=np.zeros(len0)\n",
    "    # si el numero de filas es mayor que el numero de columnas, tomo el maximo de cada columna\n",
    "    for i in range():\n",
    "        # Encuentra el valor m치ximo en la columna j\n",
    "        max_val[i] = np.max(linsim_[i, :])\n",
    "\n",
    "\n",
    "        # Encuentra la fila donde ocurre el valor m치ximo\n",
    "        j = np.argmax(linsim_[i, :])\n",
    "\n",
    "linsim_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IHLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
