{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 13.1MB/s]                    \n",
      "2024-12-10 17:11:28 INFO: Downloaded file to C:\\Users\\maric\\stanza_resources\\resources.json\n",
      "2024-12-10 17:11:28 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-12-10 17:11:30 INFO: File exists: C:\\Users\\maric\\stanza_resources\\en\\default.zip\n",
      "2024-12-10 17:11:35 INFO: Finished downloading models and saved to C:\\Users\\maric\\stanza_resources\n",
      "2024-12-10 17:11:35 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 26.9MB/s]                    \n",
      "2024-12-10 17:11:35 INFO: Downloaded file to C:\\Users\\maric\\stanza_resources\\resources.json\n",
      "2024-12-10 17:11:36 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-12-10 17:11:36 INFO: Using device: cpu\n",
      "2024-12-10 17:11:36 INFO: Loading: tokenize\n",
      "2024-12-10 17:11:36 INFO: Loading: mwt\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 17:11:36 INFO: Loading: pos\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 17:11:37 INFO: Loading: lemma\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 17:11:37 INFO: Loading: depparse\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 17:11:38 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from typing import List, Set\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from nltk.chunk import RegexpParser\n",
    "import copy\n",
    "from math import log\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError  # Import WordNetError\n",
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse', tokenize_pretokenized=True)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# Download required resource\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Micron', 'has', 'declared', 'its', 'first', ...</td>\n",
       "      <td>['Micron', \"'s\", 'numbers', 'also', 'marked', ...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['The', 'fines', 'are', 'part', 'of', 'failed'...</td>\n",
       "      <td>['Perry', 'said', 'he', 'backs', 'the', 'Senat...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['The', 'tech-loaded', 'Nasdaq', 'composite', ...</td>\n",
       "      <td>['The', 'technology-laced', 'Nasdaq', 'Composi...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...   \n",
       "1  ['Micron', 'has', 'declared', 'its', 'first', ...   \n",
       "2  ['The', 'fines', 'are', 'part', 'of', 'failed'...   \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...   \n",
       "4  ['The', 'tech-loaded', 'Nasdaq', 'composite', ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...  4.00  \n",
       "1  ['Micron', \"'s\", 'numbers', 'also', 'marked', ...  3.75  \n",
       "2  ['Perry', 'said', 'he', 'backs', 'the', 'Senat...  2.80  \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...  3.40  \n",
       "4  ['The', 'technology-laced', 'Nasdaq', 'Composi...  2.40  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_token_df = pd.read_csv('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...   4.0  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  3.75  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...   2.8  \n",
       "3  [The, American, Anglican, Council, ,, which, r...   3.4  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...   2.4  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the 2 first columns from strings to actual lists of strings\n",
    "\n",
    "train_df = pd.DataFrame(columns=['0','1','gs'], index=range(n))\n",
    "train_df.iloc[:, :2] = train_token_df.iloc[:, :2].map(ast.literal_eval)\n",
    "train_df.loc[:, 'gs'] = train_token_df.loc[:, 'gs']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(but, CC), (other, JJ), (sources, NNS), (clos...</td>\n",
       "      <td>[(but, CC), (other, JJ), (sources, NNS), (clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(micron, NN), (has, VBZ), (declared, VBN), (i...</td>\n",
       "      <td>[(micron, NN), (s, NN), (numbers, NNS), (also,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(the, DT), (fines, NNS), (are, VBP), (part, N...</td>\n",
       "      <td>[(perry, NN), (said, VBD), (he, PRP), (backs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(the, DT), (american, JJ), (anglican, NN), (c...</td>\n",
       "      <td>[(the, DT), (american, JJ), (anglican, NN), (c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(the, DT), (tech-loaded, JJ), (nasdaq, NN), (...</td>\n",
       "      <td>[(the, DT), (technology-laced, JJ), (nasdaq, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(but, CC), (other, JJ), (sources, NNS), (clos...   \n",
       "1  [(micron, NN), (has, VBZ), (declared, VBN), (i...   \n",
       "2  [(the, DT), (fines, NNS), (are, VBP), (part, N...   \n",
       "3  [(the, DT), (american, JJ), (anglican, NN), (c...   \n",
       "4  [(the, DT), (tech-loaded, JJ), (nasdaq, NN), (...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(but, CC), (other, JJ), (sources, NNS), (clos...  \n",
       "1  [(micron, NN), (s, NN), (numbers, NNS), (also,...  \n",
       "2  [(perry, NN), (said, VBD), (he, PRP), (backs, ...  \n",
       "3  [(the, DT), (american, JJ), (anglican, NN), (c...  \n",
       "4  [(the, DT), (technology-laced, JJ), (nasdaq, N...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Remove punctuation\n",
    "train_df = preprocessor.remove_punctuation(train_df)\n",
    "train_df = preprocessor.convert_to_lowercase(train_df)\n",
    "train_df = preprocessor.remove_empty_strings(train_df)\n",
    "\n",
    "# POS-tagging the words\n",
    "n=len(train_df)\n",
    "train_df_POS = pd.DataFrame(columns=['0','1'])\n",
    "\n",
    "for i in range(n):\n",
    "    train_df_POS.loc[i,'0'] = nltk.pos_tag(train_df.loc[i,'0']) \n",
    "    train_df_POS.loc[i,'1'] = nltk.pos_tag(train_df.loc[i,'1']) \n",
    "\n",
    "train_df_POS.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* N-GRAMS OVERLAP REMOVING SOME FUNCTION WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(other, JJ), (sources, NNS), (close, RB), (to...</td>\n",
       "      <td>[(other, JJ), (sources, NNS), (close, RB), (to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(Micron, NNP), (has, VBZ), (declared, VBN), (...</td>\n",
       "      <td>[(Micron, NNP), (s, NN), (numbers, NNS), (also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(fines, NNS), (are, VBP), (part, NN), (failed...</td>\n",
       "      <td>[(Perry, NNP), (said, VBD), (he, PRP), (backs,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(American, JJ), (Anglican, NNP), (Council, NN...</td>\n",
       "      <td>[(American, JJ), (Anglican, NNP), (Council, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(tech-loaded, JJ), (Nasdaq, NNP), (composite,...</td>\n",
       "      <td>[(technology-laced, JJ), (Nasdaq, NNP), (Compo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [(other, JJ), (sources, NNS), (close, RB), (to...   \n",
       "1  [(Micron, NNP), (has, VBZ), (declared, VBN), (...   \n",
       "2  [(fines, NNS), (are, VBP), (part, NN), (failed...   \n",
       "3  [(American, JJ), (Anglican, NNP), (Council, NN...   \n",
       "4  [(tech-loaded, JJ), (Nasdaq, NNP), (composite,...   \n",
       "\n",
       "                                                   1  \n",
       "0  [(other, JJ), (sources, NNS), (close, RB), (to...  \n",
       "1  [(Micron, NNP), (s, NN), (numbers, NNS), (also...  \n",
       "2  [(Perry, NNP), (said, VBD), (he, PRP), (backs,...  \n",
       "3  [(American, JJ), (Anglican, NNP), (Council, NN...  \n",
       "4  [(technology-laced, JJ), (Nasdaq, NNP), (Compo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function words (prepositions, conjunctions, articles) carry less meaning than the content words,\n",
    "# and thus removing them might eliminate the noise and provide a more accurate estimate of semantic similarity.\n",
    "\n",
    "# the use of Part of speech (POS) tagging is a critical component of syntactic analysis because it involves \n",
    "# identifying the grammatical roles of words (e.g., noun, verb, adjective) within a sentence.\n",
    "\n",
    "function_words_tag = {'IN', 'CC', 'DT', 'PDT', 'WDT'}\n",
    "\n",
    "# Create a deep copy of the DataFrame\n",
    "train_df_POS_bis = copy.deepcopy(train_df_POS)\n",
    "\n",
    "# Iterate through the rows and modify columns '0' and '1'\n",
    "for i in range(n):\n",
    "    for tag in function_words_tag:\n",
    "        # Extract, modify, and reassign the list in column '0'\n",
    "        col_0 = train_df_POS_bis.at[i, '0']\n",
    "        train_df_POS_bis.at[i, '0'] = [item for item in col_0 if item[1] != tag]\n",
    "\n",
    "        # Extract, modify, and reassign the list in column '1'\n",
    "        col_1 = train_df_POS_bis.at[i, '1']\n",
    "        train_df_POS_bis.at[i, '1'] = [item for item in col_1 if item[1] != tag]\n",
    "\n",
    "train_df_POS_bis.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[other, sources, close, to, sale, said, Vivend...</td>\n",
       "      <td>[other, sources, close, to, sale, said, Vivend...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, s, numbers, also, marked, first, quar...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fines, are, part, failed, Republican, efforts...</td>\n",
       "      <td>[Perry, said, he, backs, Senate, s, efforts, i...</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[American, Anglican, Council, represents, Epis...</td>\n",
       "      <td>[American, Anglican, Council, represents, Epis...</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tech-loaded, Nasdaq, composite, rose, 20.96, ...</td>\n",
       "      <td>[technology-laced, Nasdaq, Composite, Index, I...</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Amgen, shares, gained, 93, cents, 1.45, perce...</td>\n",
       "      <td>[Shares, Allergan, were, up, 14, cents, 78.40,...</td>\n",
       "      <td>1.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[U.S, prosecutors, have, arrested, more, 130, ...</td>\n",
       "      <td>[More, 130, people, have, been, arrested, 17, ...</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Chavez, said, investigators, feel, confident,...</td>\n",
       "      <td>[Albuquerque, Mayor, Martin, Chavez, said, inv...</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Authorities, said, scientist, properly, quara...</td>\n",
       "      <td>[scientist, also, quarantined, himself, home, ...</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[support, will, come, free, software, upgrade,...</td>\n",
       "      <td>[upgrade, will, be, available, free, download,...</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [other, sources, close, to, sale, said, Vivend...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [fines, are, part, failed, Republican, efforts...   \n",
       "3  [American, Anglican, Council, represents, Epis...   \n",
       "4  [tech-loaded, Nasdaq, composite, rose, 20.96, ...   \n",
       "5  [Amgen, shares, gained, 93, cents, 1.45, perce...   \n",
       "6  [U.S, prosecutors, have, arrested, more, 130, ...   \n",
       "7  [Chavez, said, investigators, feel, confident,...   \n",
       "8  [Authorities, said, scientist, properly, quara...   \n",
       "9  [support, will, come, free, software, upgrade,...   \n",
       "\n",
       "                                                   1     gs  \n",
       "0  [other, sources, close, to, sale, said, Vivend...    4.0  \n",
       "1  [Micron, s, numbers, also, marked, first, quar...   3.75  \n",
       "2  [Perry, said, he, backs, Senate, s, efforts, i...    2.8  \n",
       "3  [American, Anglican, Council, represents, Epis...    3.4  \n",
       "4  [technology-laced, Nasdaq, Composite, Index, I...    2.4  \n",
       "5  [Shares, Allergan, were, up, 14, cents, 78.40,...  1.333  \n",
       "6  [More, 130, people, have, been, arrested, 17, ...    4.6  \n",
       "7  [Albuquerque, Mayor, Martin, Chavez, said, inv...    3.8  \n",
       "8  [scientist, also, quarantined, himself, home, ...    4.2  \n",
       "9  [upgrade, will, be, available, free, download,...    2.6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_1 = pd.DataFrame(columns=['0','1','gs'])\n",
    "\n",
    "for i in range(n):\n",
    "    sentence=[]\n",
    "    for j in range(len(train_df_POS_bis.loc[i,'0'])):\n",
    "        sentence.append(train_df_POS_bis.loc[i,'0'][j][0])\n",
    "    train_df_1.loc[i,'0']=sentence\n",
    "    sentence=[]\n",
    "    for k in range(len(train_df_POS_bis.loc[i,'1'])):\n",
    "        sentence.append(train_df_POS_bis.loc[i,'1'][k][0])\n",
    "    train_df_1.loc[i,'1']=sentence\n",
    "\n",
    "train_df_1['gs'] = train_df['gs']\n",
    "train_df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_overlap(tokens1: List[str], tokens2: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes the n-gram overlap between two tokenized sentences.\n",
    "\n",
    "    Parameters:\n",
    "        tokens1 (List[str]): Tokenized first sentence as a list of strings.\n",
    "        tokens2 (List[str]): Tokenized second sentence as a list of strings.\n",
    "        n (int): The size of n-grams.\n",
    "\n",
    "    Returns:\n",
    "        float: The n-gram overlap ratio.\n",
    "    \"\"\"\n",
    "    def generate_ngrams(tokens: List[str], n: int) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Generates n-grams for a given list of tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        return set([' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)])\n",
    "\n",
    "    # Generate n-grams for both token lists\n",
    "    ngrams_s1 = generate_ngrams(tokens1, n)\n",
    "    ngrams_s2 = generate_ngrams(tokens2, n)\n",
    "\n",
    "    # Compute the intersection \n",
    "    intersection = ngrams_s1.intersection(ngrams_s2)\n",
    "\n",
    "    # Compute the n gram overlap when posible\n",
    "    if len(intersection)==0:\n",
    "        ngo=0\n",
    "    else:\n",
    "        ngo=2/((len(ngrams_s1)+len(ngrams_s2))/len(intersection))\n",
    "\n",
    "    return float(ngo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_tagging_unigrams</th>\n",
       "      <th>POS_tagging_bigrams</th>\n",
       "      <th>POS_tagging_trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POS_tagging_unigrams  POS_tagging_bigrams  POS_tagging_trigrams\n",
       "0              0.702703             0.594595              0.514286\n",
       "1              0.571429             0.421053              0.352941\n",
       "2              0.500000             0.250000              0.090909\n",
       "3              0.777778             0.764706              0.750000\n",
       "4              0.230769             0.000000              0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syntactic_features=pd.DataFrame(columns=['POS_tagging_unigrams','POS_tagging_bigrams','POS_tagging_trigrams'])\n",
    "\n",
    "for i in range(n):\n",
    "    # unigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_unigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],1)\n",
    "    # bigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_bigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],2)\n",
    "    # trigrams\n",
    "    syntactic_features.loc[i,'POS_tagging_trigrams'] = n_gram_overlap(train_df_1.loc[i,'0'],train_df_1.loc[i,'1'],3)\n",
    "\n",
    "\n",
    "# Convert all columns in a DataFrame to numeric, coercing errors into NaN.\n",
    "syntactic_features['POS_tagging_unigrams'] = pd.to_numeric(syntactic_features['POS_tagging_unigrams'], errors='coerce') \n",
    "syntactic_features['POS_tagging_bigrams'] = pd.to_numeric(syntactic_features['POS_tagging_bigrams'], errors='coerce') \n",
    "syntactic_features['POS_tagging_trigrams'] = pd.to_numeric(syntactic_features['POS_tagging_trigrams'], errors='coerce') \n",
    "\n",
    "syntactic_features.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SYNTACTIC ROLES SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFY WORDS IN SET OF 's','p' or 'o' AND CONVERT THE WORDS INTO LEMMAS\n",
    "\n",
    "def classify_syntactic_roles(doc):\n",
    "    roles = {'p': [], 's': [], 'o': []}  # Initialize lists for multiple sets of p, s, o   \n",
    "    for sentence in doc.sentences:        \n",
    "        # Initialize temporary sets for predicates, subjects, and objects\n",
    "        temp_p = set()  # predicates\n",
    "        subject_sets = []  # List of sets for subjects\n",
    "        object_sets = []  # List of sets for objects\n",
    "        \n",
    "        prep_objs = {}  # To store prepositional objects linked to the same predicate\n",
    "        \n",
    "        # Process each word in the sentence\n",
    "        for word in sentence.words:\n",
    "            if word.deprel == 'root':  # The predicate (main verb)\n",
    "                temp_p.add(word.text)\n",
    "            elif word.deprel in ['nsubj', 'nsubjpass']:  # Subject\n",
    "                # Check if it belongs to an existing set\n",
    "                head_text = sentence.words[word.head - 1].text if word.head > 0 else None\n",
    "                added = False\n",
    "                for s_set in subject_sets:\n",
    "                    if any(head_text == sentence.words[s.head - 1].text for s in sentence.words if s.text in s_set):\n",
    "                        s_set.add(word.text)\n",
    "                        added = True\n",
    "                        break\n",
    "                if not added:\n",
    "                    subject_sets.append({word.text})  # Create a new set\n",
    "            elif word.deprel in ['obj', 'dobj', 'obl', 'case']:  # Object\n",
    "                object_sets.append({word.text})  # Create a set for this object\n",
    "            elif word.deprel == 'conj':  # Conjunction linking words\n",
    "                head = sentence.words[word.head - 1].text  # The word it is conjoined with\n",
    "                # If head is a subject, add the conjunct word to the subject set\n",
    "                for s_set in subject_sets:\n",
    "                    if head in s_set:\n",
    "                        s_set.add(word.text)\n",
    "                        break\n",
    "                # If head is an object, add the conjunct word to the object set\n",
    "                for obj_set in object_sets:\n",
    "                    if head in obj_set:\n",
    "                        obj_set.add(word.text)\n",
    "                        break\n",
    "                # If head is a predicate, add the conjunct word to the predicate set\n",
    "                if head in temp_p: # coordinated structure\n",
    "                    temp_p.add(word.text)\n",
    "            elif word.deprel == 'ccomp': # subordinate clausures\n",
    "                head = sentence.words[word.head - 1].text  # The word it is conjoined with\n",
    "                if head in temp_p:\n",
    "                    temp_p.add(word.text)\n",
    "\n",
    "        # EXTRA: Merge object sets based on the specified rules\n",
    "        merged_object_sets = []\n",
    "        while object_sets:\n",
    "            current_set = object_sets.pop(0)\n",
    "            merged = False\n",
    "            for other_set in object_sets:\n",
    "                # Rule 1: Check if any 'case' word's head is in another set\n",
    "                for word in current_set:\n",
    "                    for other_word in sentence.words:\n",
    "                        if other_word.text == word and other_word.deprel == 'case':\n",
    "                            head_word = sentence.words[other_word.head - 1] if other_word.head > 0 else None\n",
    "                            if head_word and any(head_word.text in s for s in object_sets):\n",
    "                                other_set.update(current_set)\n",
    "                                merged = True\n",
    "                                break\n",
    "                    if merged:\n",
    "                        break\n",
    "                if merged:\n",
    "                    break\n",
    "                # Check if two words in different sets share the same head\n",
    "                for word1 in current_set:\n",
    "                    for word2 in other_set:\n",
    "                        word1_head = next((w.head for w in sentence.words if w.text == word1), None)\n",
    "                        word2_head = next((w.head for w in sentence.words if w.text == word2), None)\n",
    "                        if word1_head and word2_head and word1_head == word2_head:\n",
    "                            other_set.update(current_set)\n",
    "                            merged = True\n",
    "                            break\n",
    "                if merged:\n",
    "                    break\n",
    "            if not merged:\n",
    "                merged_object_sets.append(current_set)\n",
    "\n",
    "        # Assign to the roles dictionary\n",
    "        for predicate in temp_p:\n",
    "            roles['p'].append({predicate})  # Create a set for each predicate\n",
    "        \n",
    "        for s_set in subject_sets:\n",
    "            roles['s'].append(s_set)  # Group subjects as their respective sets\n",
    "        \n",
    "        for obj_set in merged_object_sets:\n",
    "            roles['o'].append(obj_set)  # Group objects as their respective sets\n",
    "\n",
    "            # Replace words in the sets with their lemmas\n",
    "        for role in roles:\n",
    "            for idx, word_set in enumerate(roles[role]):\n",
    "                roles[role][idx] = {sentence.words[next(i for i, w in enumerate(sentence.words) if w.text == word)].lemma \n",
    "                                    for word in word_set}\n",
    "\n",
    "        return roles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain a synset from a lemma\n",
    "def extract_synset(w):\n",
    "    # Wordnet tags\n",
    "  d = {'NN': 'n', 'NNS': 'n',\n",
    "        'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
    "        'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
    "        'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
    "  pair=nltk.pos_tag([w])\n",
    "  if pair[0][1] in d.keys(): # Check if has a wordnet tag\n",
    "    word_synsets = wn.synsets(w,d[pair[0][1]])\n",
    "    if len(word_synsets)==0: # if the list of synsets is empty (e.g. proper nouns)\n",
    "      return None \n",
    "    else: \n",
    "      return word_synsets[0]\n",
    "  \n",
    "  else:\n",
    "    # 'The lemma w has no wordnet tag\n",
    "    return None\n",
    "\n",
    "# function to compute the similarity between two chunks\n",
    "def chunksim(c1,c2):\n",
    "    # compute lin similarity first\n",
    "    sim_score=0\n",
    "    for l1 in c1:\n",
    "        for l2 in c2:\n",
    "            synset_l1=extract_synset(l1)\n",
    "            synset_l2=extract_synset(l2)\n",
    "\n",
    "            if synset_l1 is None or synset_l2 is None:\n",
    "               continue\n",
    "\n",
    "            elif synset_l1.pos()==synset_l2.pos():\n",
    "                try:\n",
    "                  # Calculate Lin Similarity\n",
    "                  sim_score += synset_l1.lin_similarity(synset_l2, brown_ic)\n",
    "                except WordNetError as e:\n",
    "                  continue\n",
    "\n",
    "    if sim_score==0:\n",
    "        return 0\n",
    "    else:\n",
    "        ckn1=sim_score/len(c1)\n",
    "        ckn2=sim_score/len(c2)\n",
    "\n",
    "        return 2*ckn1*ckn2/(ckn1+ckn2) #harmonic mean \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain the chunk similarity for the pair of chunks of the same type with maximum similarity\n",
    "def compute_similarity_chunks(predicates_0, predicates_1):\n",
    "    \n",
    "    len0=len(predicates_0)\n",
    "    len1=len(predicates_1)\n",
    "\n",
    "    if len0==0 or len1==0:\n",
    "        return 0\n",
    "\n",
    "    sim=0\n",
    "\n",
    "    linsim_ = np.zeros((len0,len1))\n",
    "\n",
    "    for i in range(len0):\n",
    "        for j in range(len1):\n",
    "            linsim_[i,j] = chunksim(predicates_0[i],predicates_1[j])\n",
    "\n",
    "    if len0 > len1:\n",
    "        max_val=np.zeros(len1)\n",
    "        # if num columns > num of rows, then the number of max is equal the num of columns\n",
    "        for j in range(len1):\n",
    "            # I find the actual and valid max value and its coordinates\n",
    "\n",
    "            max_val[j] = np.max(linsim_[:, :])\n",
    "            max_coords = np.unravel_index(np.argmax(linsim_), linsim_.shape)\n",
    "\n",
    "            # the column where the max value was founded becomes 0\n",
    "            linsim_[:,max_coords[1]]=0\n",
    "        # I compute the mean of all the pair of p\n",
    "        sim=np.mean(max_val)\n",
    "        \n",
    "    else:\n",
    "        max_val=np.zeros(len0)\n",
    "        # if num rows > num of columns, then the number of max is equal the num of columns\n",
    "        for i in range(len0):\n",
    "            # I find the actual and valid max value and its coordinates\n",
    "            max_val[i] = np.max(linsim_[:, :])\n",
    "            max_coords = np.unravel_index(np.argmax(linsim_), linsim_.shape)\n",
    "\n",
    "            # the row where the max value was founded becomes 0\n",
    "            linsim_[max_coords[0],:]=0\n",
    "            \n",
    "        # I compute the mean of all the pair of p\n",
    "        sim=np.mean(max_val)\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_tagging_unigrams</th>\n",
       "      <th>POS_tagging_bigrams</th>\n",
       "      <th>POS_tagging_trigrams</th>\n",
       "      <th>chunk_sim_p</th>\n",
       "      <th>chunk_sim_s</th>\n",
       "      <th>chunk_sim_o</th>\n",
       "      <th>total_sim_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.174853</td>\n",
       "      <td>0.558284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.722495</td>\n",
       "      <td>0.240832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.360534</td>\n",
       "      <td>0.564623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.304668</td>\n",
       "      <td>0.101556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POS_tagging_unigrams  POS_tagging_bigrams  POS_tagging_trigrams  \\\n",
       "0              0.702703             0.594595              0.514286   \n",
       "1              0.571429             0.421053              0.352941   \n",
       "2              0.500000             0.250000              0.090909   \n",
       "3              0.777778             0.764706              0.750000   \n",
       "4              0.230769             0.000000              0.000000   \n",
       "\n",
       "  chunk_sim_p chunk_sim_s chunk_sim_o total_sim_chunks  \n",
       "0         1.0         0.5    0.174853         0.558284  \n",
       "1         0.0         0.0    0.722495         0.240832  \n",
       "2         0.0         0.0         0.0              0.0  \n",
       "3         1.0    0.333333    0.360534         0.564623  \n",
       "4         0.0         0.0    0.304668         0.101556  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the chunk similarity for the dataset \n",
    "\n",
    "chunk_sim_df = pd.DataFrame(columns=['chunk_sim_p','chunk_sim_s','chunk_sim_o','total_sim_chunks'], index=range(n))\n",
    "\n",
    "for k in range(n):\n",
    "    # Classify the words into p, s, o for each sentence \n",
    "    roles_0 = classify_syntactic_roles(nlp_stanza([train_df.loc[k,'0']]))\n",
    "    roles_1 = classify_syntactic_roles(nlp_stanza([train_df.loc[k,'1']]))\n",
    "    # compute the simchunk for each pair of sentence and save only the sim value \n",
    "    chunk_sim_df.loc[k,'chunk_sim_p']=compute_similarity_chunks(roles_0['p'], roles_1['p'])\n",
    "    chunk_sim_df.loc[k,'chunk_sim_s']=compute_similarity_chunks(roles_0['s'], roles_1['s'])\n",
    "    chunk_sim_df.loc[k,'chunk_sim_o']=compute_similarity_chunks(roles_0['o'], roles_1['o'])\n",
    "    chunk_sim_df.loc[k,'total_sim_chunks'] = chunk_sim_df[['chunk_sim_p','chunk_sim_s','chunk_sim_o']].iloc[k].mean(axis=0)\n",
    "\n",
    "syntactic_features['chunk_sim_p']=chunk_sim_df['chunk_sim_p']\n",
    "syntactic_features['chunk_sim_s']=chunk_sim_df['chunk_sim_s']\n",
    "syntactic_features['chunk_sim_o']=chunk_sim_df['chunk_sim_o']\n",
    "syntactic_features['total_sim_chunks']=chunk_sim_df['total_sim_chunks']\n",
    "\n",
    "syntactic_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SYNTACTIC DEPENDENCIES OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_relations(tokens):\n",
    "    \"\"\"\n",
    "    Extract dependency relations between lemmas for a list of tokenized words.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokenized words to analyze\n",
    "        nlp (stanza.Pipeline, optional): Pre-initialized Stanza pipeline. \n",
    "                                         If None, will create a new English pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing dependency relations with:\n",
    "            - 'type': dependency relation type\n",
    "            - 'governor_lemma': lemma of the governing word\n",
    "            - 'dependent_lemma': lemma of the dependent word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the tokenized input\n",
    "    doc = nlp_stanza(tokens)\n",
    "    \n",
    "    # List to store dependency relations\n",
    "    relations = []\n",
    "    \n",
    "    # Extract dependency relations from the first (and typically only) sentence\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            # Skip root word (which has head 0)\n",
    "            if word.head != 0:\n",
    "                # Find the governor (head) word\n",
    "                governor = sent.words[word.head - 1]\n",
    "                \n",
    "                # Add relation to list\n",
    "                relations.append({\n",
    "                    'type': word.deprel,\n",
    "                    'governor_lemma': governor.lemma,\n",
    "                    'dependent_lemma': word.lemma\n",
    "                })\n",
    "    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the max IC between the goberning word and the dependent from a list of dependency relations and sum all of them\n",
    "\n",
    "def compute_max_ic_sum_with_wordnet_ic(dependency_relations):\n",
    "    \"\"\"\n",
    "    Computes the sum of the maximum information content (IC) for each dependency relation\n",
    "    from a list of dependency relations, using WordNet's IC resource.\n",
    "    \n",
    "    Args:\n",
    "        dependency_relations (list of dicts): List of dependency relations, each represented by a dictionary \n",
    "                                               with 'governor_lemma' and 'dependent_lemma'.\n",
    "        \n",
    "    Returns:\n",
    "        float: The sum of maximum IC values for all dependency relations.\n",
    "    \"\"\"\n",
    "    # function to compute IC using WordNet's IC\n",
    "    def compute_ic(word):\n",
    "        synset = extract_synset(word)\n",
    "\n",
    "        # Check if synset is None\n",
    "        if synset is None:\n",
    "            return 0  # Return default value for None synsets\n",
    "\n",
    "        # Extract the part of speech\n",
    "        pos = synset.pos()  # 'n', 'v', etc.\n",
    "        if pos not in brown_ic:\n",
    "            return 0  # POS not supported in IC data\n",
    "\n",
    "        # Normalize the frequency to compute probability\n",
    "        try:\n",
    "            raw_count = brown_ic[pos][synset.offset()]\n",
    "            total_count = sum(brown_ic[pos].values())  # Total frequency for this POS\n",
    "            probability = raw_count / total_count\n",
    "            if probability <= 0 or probability > 1:\n",
    "                return 0\n",
    "            ic = -log(probability)\n",
    "            return ic\n",
    "        except KeyError:\n",
    "            return 0  # Synset not found in IC data\n",
    "            \n",
    "    # Process each dependency relation and compute the sum of max IC\n",
    "    max_ic_sum = 0\n",
    "    for relation in dependency_relations:\n",
    "        governor = relation['governor_lemma']\n",
    "        dependent = relation['dependent_lemma']\n",
    "        \n",
    "        # Compute IC for governing and dependent words using WordNet's IC\n",
    "        ic_governing = compute_ic(governor)\n",
    "        ic_dependent = compute_ic(dependent)\n",
    "        \n",
    "        # Save the maximum IC and add to the sum\n",
    "        max_ic = max(ic_governing, ic_dependent)\n",
    "        max_ic_sum += max_ic\n",
    "    \n",
    "    return max_ic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dep_overlap = pd.DataFrame(columns=['sim_dependencies'], index=range(n))\n",
    "\n",
    "\n",
    "for k in range(n):\n",
    "    relations_0 = get_dependency_relations([train_df.loc[k,'0']])\n",
    "    relations_1 = get_dependency_relations([train_df.loc[k,'1']])\n",
    "    # Compute the intersection\n",
    "    relations_intersection = [item for item in relations_0 if item in relations_1]\n",
    "    max_dependency_relations_0 = compute_max_ic_sum_with_wordnet_ic(relations_0)\n",
    "    max_dependency_relations_1 = compute_max_ic_sum_with_wordnet_ic(relations_1)\n",
    "    max_dependency_intersection = compute_max_ic_sum_with_wordnet_ic(relations_intersection)\n",
    "    if max_dependency_relations_0 == 0:\n",
    "        wdrx_1_0 = 0\n",
    "    else:\n",
    "        wdrc_1_0 = max_dependency_intersection/max_dependency_relations_0\n",
    "    if max_dependency_relations_1 == 0:\n",
    "        wdrc_0_1 = 0\n",
    "    else:\n",
    "        wdrc_0_1 = max_dependency_intersection/max_dependency_relations_1\n",
    "    sim_dep_overlap.loc[k,'sim_dep_overlap'] = (wdrc_1_0 + wdrc_0_1)/2\n",
    "\n",
    "syntactic_features['sim_dependencies'] = sim_dep_overlap['sim_dependencies']\n",
    "syntactic_features.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IHLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
