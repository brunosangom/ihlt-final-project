{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from nltk.wsd import lesk\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "from itertools import chain\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet_ic', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the Information Content (IC) corpus\n",
    "ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = preprocessor.load_dataset('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Normalize the text\n",
    "normal_train_df = preprocessor.remove_punctuation(train_df)\n",
    "normal_train_df = preprocessor.convert_to_lowercase(normal_train_df)\n",
    "normal_train_df = preprocessor.remove_empty_strings(normal_train_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_train_df = preprocessor.remove_stopwords(normal_train_df)\n",
    "lemmas_train_df = preprocessor.lemmatize(sw_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the 3 DataFrames into a Dictionary, with their \"names\"\n",
    "train_dfs = {'normal': normal_train_df, 'sw': sw_train_df, 'lemmas': lemmas_train_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "train_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy String Tiling (GST)\n",
    "\n",
    "# Apply Greedy String Tiling to find maximal matching substrings (tiles) between two tokenized sentences.\n",
    "def greedy_string_tiling(sentence1_tokens, sentence2_tokens, min_match_length=2):\n",
    "    # Convert tokenized sentences to full sentences\n",
    "    sentence1 = \" \".join(sentence1_tokens)\n",
    "    sentence2 = \" \".join(sentence2_tokens)\n",
    "\n",
    "    matched_indices1 = set()\n",
    "    matched_indices2 = set()\n",
    "    tile_lengths = []\n",
    "\n",
    "    while True:\n",
    "        max_tile = None\n",
    "        max_length = 0\n",
    "\n",
    "        # Find the longest match not covered by existing tiles\n",
    "        for i in range(len(sentence1)):\n",
    "            for j in range(len(sentence2)):\n",
    "                length = 0\n",
    "                while (\n",
    "                    i + length < len(sentence1) and\n",
    "                    j + length < len(sentence2) and\n",
    "                    sentence1[i + length] == sentence2[j + length] and\n",
    "                    (i + length not in matched_indices1) and\n",
    "                    (j + length not in matched_indices2)\n",
    "                ):\n",
    "                    length += 1\n",
    "\n",
    "                if length >= min_match_length and length > max_length:\n",
    "                    max_tile = (i, j, length)\n",
    "                    max_length = length\n",
    "\n",
    "        # If no valid tile is found, stop\n",
    "        if not max_tile:\n",
    "            break\n",
    "\n",
    "        # Mark the matched indices as covered\n",
    "        start1, start2, length = max_tile\n",
    "        for k in range(length):\n",
    "            matched_indices1.add(start1 + k)\n",
    "            matched_indices2.add(start2 + k)\n",
    "\n",
    "        tile_lengths.append(max_tile[2])\n",
    "\n",
    "    # Aggregate all tile lengths by summing and normalizing by sentence length\n",
    "    return np.sum(tile_lengths) / max(len(sentence1), len(sentence2))\n",
    "\n",
    "gst_min_lengths = [5, 10]\n",
    "\n",
    "def compute_greedy_string_tiling(dfs, features_df):\n",
    "    for min_match_length in gst_min_lengths:\n",
    "        for name, df in dfs.items():\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(tqdm(\n",
    "                    executor.map(\n",
    "                        lambda sentence_pair: greedy_string_tiling(sentence_pair['0'], sentence_pair['1'], min_match_length),\n",
    "                        (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                    ),\n",
    "                    total=len(df),\n",
    "                    desc=f\"Computing Greedy String Tiling {name}, {min_match_length}\"\n",
    "                ))\n",
    "            features_df[f'{name}_gst_{min_match_length}'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Greedy String Tiling normal, 5: 100%|██████████| 2234/2234 [01:25<00:00, 26.24it/s] \n",
      "Computing Greedy String Tiling sw, 5: 100%|██████████| 2234/2234 [00:27<00:00, 80.74it/s]\n",
      "Computing Greedy String Tiling lemmas, 5: 100%|██████████| 2234/2234 [00:27<00:00, 81.54it/s]\n",
      "Computing Greedy String Tiling normal, 10: 100%|██████████| 2234/2234 [00:53<00:00, 41.46it/s] \n",
      "Computing Greedy String Tiling sw, 10: 100%|██████████| 2234/2234 [00:18<00:00, 122.12it/s]\n",
      "Computing Greedy String Tiling lemmas, 10: 100%|██████████| 2234/2234 [00:15<00:00, 140.21it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_greedy_string_tiling(train_dfs, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character n-grams Cosine Similarity\n",
    "\n",
    "# Generate character n-grams for a tokenized sentence.\n",
    "def char_ngrams(sent_tokens, n):\n",
    "    sentence = ' '.join(sent_tokens)  # Join tokens into a single string\n",
    "    return [''.join(sentence[i:i+n]) for i in range(len(sentence) - n + 1)]\n",
    "\n",
    "# Compute the cosine similarity between two tokenized sentences based on character n-grams.\n",
    "def character_ngram_similarity(sent1_tokens, sent2_tokens, n=3):\n",
    "    # Generate character n-grams for both sentences\n",
    "    sent1_ngrams = char_ngrams(sent1_tokens, n)\n",
    "    sent2_ngrams = char_ngrams(sent2_tokens, n)\n",
    "\n",
    "    # Combine n-grams into a single string per sentence\n",
    "    sent1_ngram_str = ' '.join(sent1_ngrams)\n",
    "    sent2_ngram_str = ' '.join(sent2_ngrams)\n",
    "\n",
    "    # Initialize TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Convert the n-gram strings into TF-IDF representations\n",
    "    tfidf_matrix = vectorizer.fit_transform([sent1_ngram_str, sent2_ngram_str])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "ngram_lengths = [2, 3, 4]\n",
    "\n",
    "def compute_character_ngram_similarity(dfs, features_df):\n",
    "    for n in ngram_lengths:\n",
    "        for name, df in dfs.items():\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(tqdm(\n",
    "                    executor.map(\n",
    "                        lambda sentence_pair: character_ngram_similarity(sentence_pair['0'], sentence_pair['1'], n),\n",
    "                        (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                    ),\n",
    "                    total=len(df),\n",
    "                    desc=f\"Computing Character n-gram Similarity {name}, {n}\"\n",
    "                ))\n",
    "            features_df[f'{name}_gst_{n}'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Character n-gram Similarity normal, 2: 100%|██████████| 2234/2234 [00:04<00:00, 542.75it/s]\n",
      "Computing Character n-gram Similarity sw, 2: 100%|██████████| 2234/2234 [00:03<00:00, 610.22it/s]\n",
      "Computing Character n-gram Similarity lemmas, 2: 100%|██████████| 2234/2234 [00:03<00:00, 688.30it/s]\n",
      "Computing Character n-gram Similarity normal, 3: 100%|██████████| 2234/2234 [00:04<00:00, 470.02it/s]\n",
      "Computing Character n-gram Similarity sw, 3: 100%|██████████| 2234/2234 [00:04<00:00, 452.72it/s]\n",
      "Computing Character n-gram Similarity lemmas, 3: 100%|██████████| 2234/2234 [00:04<00:00, 542.68it/s]\n",
      "Computing Character n-gram Similarity normal, 4: 100%|██████████| 2234/2234 [00:04<00:00, 500.17it/s] \n",
      "Computing Character n-gram Similarity sw, 4: 100%|██████████| 2234/2234 [00:03<00:00, 579.86it/s]\n",
      "Computing Character n-gram Similarity lemmas, 4: 100%|██████████| 2234/2234 [00:03<00:00, 574.02it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_character_ngram_similarity(train_dfs, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a CSV with the Normalized Gold Standard\n",
    "train_features_df['gs'] = train_df['gs'] / 5.0\n",
    "\n",
    "train_features_df.to_csv('train/stringFeatures_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Greedy String Tiling normal, 5: 100%|██████████| 3108/3108 [00:16<00:00, 188.07it/s]\n",
      "Computing Greedy String Tiling sw, 5: 100%|██████████| 3108/3108 [00:06<00:00, 466.16it/s]\n",
      "Computing Greedy String Tiling lemmas, 5: 100%|██████████| 3108/3108 [00:05<00:00, 617.52it/s]\n",
      "Computing Greedy String Tiling normal, 10: 100%|██████████| 3108/3108 [00:04<00:00, 720.93it/s]\n",
      "Computing Greedy String Tiling sw, 10: 100%|██████████| 3108/3108 [00:02<00:00, 1052.39it/s]\n",
      "Computing Greedy String Tiling lemmas, 10: 100%|██████████| 3108/3108 [00:03<00:00, 1026.20it/s]\n",
      "Computing Character n-gram Similarity normal, 2: 100%|██████████| 3108/3108 [00:04<00:00, 624.94it/s]\n",
      "Computing Character n-gram Similarity sw, 2: 100%|██████████| 3108/3108 [00:05<00:00, 568.13it/s]\n",
      "Computing Character n-gram Similarity lemmas, 2: 100%|██████████| 3108/3108 [00:05<00:00, 586.87it/s]\n",
      "Computing Character n-gram Similarity normal, 3: 100%|██████████| 3108/3108 [00:06<00:00, 506.98it/s]\n",
      "Computing Character n-gram Similarity sw, 3: 100%|██████████| 3108/3108 [00:02<00:00, 1434.31it/s]\n",
      "Computing Character n-gram Similarity lemmas, 3: 100%|██████████| 3108/3108 [00:03<00:00, 857.85it/s] \n",
      "Computing Character n-gram Similarity normal, 4: 100%|██████████| 3108/3108 [00:05<00:00, 560.00it/s]\n",
      "Computing Character n-gram Similarity sw, 4: 100%|██████████| 3108/3108 [00:05<00:00, 582.15it/s]\n",
      "Computing Character n-gram Similarity lemmas, 4: 100%|██████████| 3108/3108 [00:05<00:00, 568.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the same pipeline to the testing Dataset\n",
    "\n",
    "# Load the testing dataset\n",
    "test_df = preprocessor.load_dataset('../Preprocessing/STS_test.csv')\n",
    "\n",
    "# Normalize the text\n",
    "normal_test_df = preprocessor.remove_punctuation(test_df)\n",
    "normal_test_df = preprocessor.convert_to_lowercase(normal_test_df)\n",
    "normal_test_df = preprocessor.remove_empty_strings(normal_test_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_test_df = preprocessor.remove_stopwords(normal_test_df)\n",
    "lemmas_test_df = preprocessor.lemmatize(sw_test_df)\n",
    "\n",
    "# Group the 3 DataFrames into a Dictionary, with their \"names\"\n",
    "test_dfs = {'normal': normal_test_df, 'sw': sw_test_df, 'lemmas': lemmas_test_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "test_features_df = pd.DataFrame()\n",
    "\n",
    "# Greedy String Tiling\n",
    "compute_greedy_string_tiling(test_dfs, test_features_df)\n",
    "\n",
    "# Character n-gram Cosine Similarity\n",
    "compute_character_ngram_similarity(test_dfs, test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a CSV with the Normalized Gold Standard\n",
    "test_features_df['gs'] = test_df['gs'] / 5.0\n",
    "\n",
    "test_features_df.to_csv('test/stringFeatures_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Variable  Correlation\n",
      "0    normal_gst_5     0.571769\n",
      "1        sw_gst_5     0.666634\n",
      "2    lemmas_gst_5     0.666148\n",
      "3   normal_gst_10     0.526147\n",
      "4       sw_gst_10     0.560675\n",
      "5   lemmas_gst_10     0.560554\n",
      "6    normal_gst_2     0.721553\n",
      "7        sw_gst_2     0.687611\n",
      "8    lemmas_gst_2     0.690158\n",
      "9    normal_gst_3     0.651404\n",
      "10       sw_gst_3     0.643363\n",
      "11   lemmas_gst_3     0.647888\n",
      "12   normal_gst_4     0.577053\n",
      "13       sw_gst_4     0.635147\n",
      "14   lemmas_gst_4     0.642041\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = {}\n",
    "for column in train_features_df.columns:\n",
    "    corr, _ = pearsonr(train_features_df[column], train_df['gs'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "# Convert the dictionary to a DataFrame for tabular representation\n",
    "correlation_table = pd.DataFrame(list(correlations.items()), columns=['Variable', 'Correlation'])\n",
    "\n",
    "print(correlation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
