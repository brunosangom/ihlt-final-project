{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet_ic', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = preprocessor.load_dataset('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Normalize the text\n",
    "normal_train_df = preprocessor.remove_punctuation(train_df)\n",
    "normal_train_df = preprocessor.convert_to_lowercase(normal_train_df)\n",
    "normal_train_df = preprocessor.remove_empty_strings(normal_train_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_train_df = preprocessor.remove_stopwords(normal_train_df)\n",
    "lemmas_train_df = preprocessor.lemmatize(sw_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the 3 DataFrames into a Dictionary, with their \"names\"\n",
    "train_dfs = {'normal': normal_train_df, 'sw': sw_train_df, 'lemmas': lemmas_train_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "train_features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy String Tiling (GST)\n",
    "\n",
    "# Apply Greedy String Tiling to find maximal matching substrings (tiles) between two tokenized sentences.\n",
    "def greedy_string_tiling(sentence1_tokens, sentence2_tokens, min_match_length=2):\n",
    "    # Convert tokenized sentences to full sentences\n",
    "    sentence1 = \" \".join(sentence1_tokens)\n",
    "    sentence2 = \" \".join(sentence2_tokens)\n",
    "\n",
    "    matched_indices1 = set()\n",
    "    matched_indices2 = set()\n",
    "    tile_lengths = []\n",
    "\n",
    "    while True:\n",
    "        max_tile = None\n",
    "        max_length = 0\n",
    "\n",
    "        # Find the longest match not covered by existing tiles\n",
    "        for i in range(len(sentence1)):\n",
    "            for j in range(len(sentence2)):\n",
    "                length = 0\n",
    "                while (\n",
    "                    i + length < len(sentence1) and\n",
    "                    j + length < len(sentence2) and\n",
    "                    sentence1[i + length] == sentence2[j + length] and\n",
    "                    (i + length not in matched_indices1) and\n",
    "                    (j + length not in matched_indices2)\n",
    "                ):\n",
    "                    length += 1\n",
    "\n",
    "                if length >= min_match_length and length > max_length:\n",
    "                    max_tile = (i, j, length)\n",
    "                    max_length = length\n",
    "\n",
    "        # If no valid tile is found, stop\n",
    "        if not max_tile:\n",
    "            break\n",
    "\n",
    "        # Mark the matched indices as covered\n",
    "        start1, start2, length = max_tile\n",
    "        for k in range(length):\n",
    "            matched_indices1.add(start1 + k)\n",
    "            matched_indices2.add(start2 + k)\n",
    "\n",
    "        tile_lengths.append(max_tile[2])\n",
    "\n",
    "    # Aggregate all tile lengths by summing and normalizing by sentence length\n",
    "    return np.sum(tile_lengths) / max(len(sentence1), len(sentence2))\n",
    "\n",
    "gst_min_lengths = [5, 10]\n",
    "\n",
    "def compute_greedy_string_tiling(dfs, features_df):\n",
    "    for min_match_length in gst_min_lengths:\n",
    "        for name, df in dfs.items():\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(tqdm(\n",
    "                    executor.map(\n",
    "                        lambda sentence_pair: greedy_string_tiling(sentence_pair['0'], sentence_pair['1'], min_match_length),\n",
    "                        (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                    ),\n",
    "                    total=len(df),\n",
    "                    desc=f\"Computing Greedy String Tiling {name}, {min_match_length}\"\n",
    "                ))\n",
    "            features_df[f'{name}_gst_{min_match_length}'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Greedy String Tiling normal, 5: 100%|██████████| 2234/2234 [01:25<00:00, 26.05it/s] \n",
      "Computing Greedy String Tiling sw, 5: 100%|██████████| 2234/2234 [00:21<00:00, 103.00it/s]\n",
      "Computing Greedy String Tiling lemmas, 5: 100%|██████████| 2234/2234 [00:25<00:00, 87.20it/s] \n",
      "Computing Greedy String Tiling normal, 10: 100%|██████████| 2234/2234 [00:49<00:00, 44.73it/s] \n",
      "Computing Greedy String Tiling sw, 10: 100%|██████████| 2234/2234 [00:36<00:00, 60.74it/s] \n",
      "Computing Greedy String Tiling lemmas, 10: 100%|██████████| 2234/2234 [00:16<00:00, 136.67it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_greedy_string_tiling(train_dfs, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character n-grams Cosine Similarity\n",
    "\n",
    "# Generate character n-grams for a tokenized sentence.\n",
    "def char_ngrams(sent_tokens, n):\n",
    "    sentence = ' '.join(sent_tokens)  # Join tokens into a single string\n",
    "    return [''.join(sentence[i:i+n]) for i in range(len(sentence) - n + 1)]\n",
    "\n",
    "# Compute the cosine similarity between two tokenized sentences based on character n-grams.\n",
    "def character_ngram_similarity(sent1_tokens, sent2_tokens, n=3):\n",
    "    # Generate character n-grams for both sentences\n",
    "    sent1_ngrams = char_ngrams(sent1_tokens, n)\n",
    "    sent2_ngrams = char_ngrams(sent2_tokens, n)\n",
    "\n",
    "    # Combine n-grams into a single string per sentence\n",
    "    sent1_ngram_str = ' '.join(sent1_ngrams)\n",
    "    sent2_ngram_str = ' '.join(sent2_ngrams)\n",
    "\n",
    "    # Initialize TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Convert the n-gram strings into TF-IDF representations\n",
    "    tfidf_matrix = vectorizer.fit_transform([sent1_ngram_str, sent2_ngram_str])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "ngram_lengths = [2, 3, 4, 5]\n",
    "\n",
    "def compute_character_ngram_similarity(dfs, features_df):\n",
    "    for n in ngram_lengths:\n",
    "        for name, df in dfs.items():\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(tqdm(\n",
    "                    executor.map(\n",
    "                        lambda sentence_pair: character_ngram_similarity(sentence_pair['0'], sentence_pair['1'], n),\n",
    "                        (sentence_pair for _, sentence_pair in df.iterrows())\n",
    "                    ),\n",
    "                    total=len(df),\n",
    "                    desc=f\"Computing Character n-gram Similarity {name}, {n}\"\n",
    "                ))\n",
    "            features_df[f'{name}_char_{n}gram'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Character n-gram Similarity normal, 2: 100%|██████████| 2234/2234 [00:03<00:00, 652.44it/s] \n",
      "Computing Character n-gram Similarity sw, 2: 100%|██████████| 2234/2234 [00:03<00:00, 571.43it/s]\n",
      "Computing Character n-gram Similarity lemmas, 2: 100%|██████████| 2234/2234 [00:02<00:00, 838.05it/s]\n",
      "Computing Character n-gram Similarity normal, 3: 100%|██████████| 2234/2234 [00:03<00:00, 570.53it/s] \n",
      "Computing Character n-gram Similarity sw, 3: 100%|██████████| 2234/2234 [00:04<00:00, 552.44it/s] \n",
      "Computing Character n-gram Similarity lemmas, 3: 100%|██████████| 2234/2234 [00:03<00:00, 625.02it/s]\n",
      "Computing Character n-gram Similarity normal, 4: 100%|██████████| 2234/2234 [00:04<00:00, 513.35it/s]\n",
      "Computing Character n-gram Similarity sw, 4: 100%|██████████| 2234/2234 [00:04<00:00, 553.82it/s] \n",
      "Computing Character n-gram Similarity lemmas, 4: 100%|██████████| 2234/2234 [00:03<00:00, 609.62it/s] \n",
      "Computing Character n-gram Similarity normal, 5: 100%|██████████| 2234/2234 [00:04<00:00, 490.33it/s]\n",
      "Computing Character n-gram Similarity sw, 5: 100%|██████████| 2234/2234 [00:03<00:00, 599.76it/s]\n",
      "Computing Character n-gram Similarity lemmas, 5: 100%|██████████| 2234/2234 [00:04<00:00, 541.06it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_character_ngram_similarity(train_dfs, train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a CSV with the Normalized Gold Standard\n",
    "train_features_df['gs'] = train_df['gs'] / 5.0\n",
    "\n",
    "train_features_df.to_csv('train/stringFeatures_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Greedy String Tiling normal, 5: 100%|██████████| 3108/3108 [00:13<00:00, 238.43it/s]\n",
      "Computing Greedy String Tiling sw, 5: 100%|██████████| 3108/3108 [00:06<00:00, 476.63it/s]\n",
      "Computing Greedy String Tiling lemmas, 5: 100%|██████████| 3108/3108 [00:08<00:00, 362.67it/s]\n",
      "Computing Greedy String Tiling normal, 10: 100%|██████████| 3108/3108 [00:28<00:00, 107.50it/s]\n",
      "Computing Greedy String Tiling sw, 10: 100%|██████████| 3108/3108 [00:01<00:00, 1689.81it/s]\n",
      "Computing Greedy String Tiling lemmas, 10: 100%|██████████| 3108/3108 [00:01<00:00, 2396.25it/s]\n",
      "Computing Character n-gram Similarity normal, 2: 100%|██████████| 3108/3108 [00:04<00:00, 621.89it/s]\n",
      "Computing Character n-gram Similarity sw, 2: 100%|██████████| 3108/3108 [00:02<00:00, 1337.53it/s] \n",
      "Computing Character n-gram Similarity lemmas, 2: 100%|██████████| 3108/3108 [00:03<00:00, 874.14it/s] \n",
      "Computing Character n-gram Similarity normal, 3: 100%|██████████| 3108/3108 [00:04<00:00, 674.87it/s]\n",
      "Computing Character n-gram Similarity sw, 3: 100%|██████████| 3108/3108 [00:04<00:00, 705.45it/s]\n",
      "Computing Character n-gram Similarity lemmas, 3: 100%|██████████| 3108/3108 [00:05<00:00, 603.03it/s]\n",
      "Computing Character n-gram Similarity normal, 4: 100%|██████████| 3108/3108 [00:05<00:00, 594.06it/s]\n",
      "Computing Character n-gram Similarity sw, 4: 100%|██████████| 3108/3108 [00:05<00:00, 591.65it/s]\n",
      "Computing Character n-gram Similarity lemmas, 4: 100%|██████████| 3108/3108 [00:05<00:00, 567.00it/s]\n",
      "Computing Character n-gram Similarity normal, 5: 100%|██████████| 3108/3108 [00:05<00:00, 564.03it/s]\n",
      "Computing Character n-gram Similarity sw, 5: 100%|██████████| 3108/3108 [00:05<00:00, 530.19it/s]\n",
      "Computing Character n-gram Similarity lemmas, 5: 100%|██████████| 3108/3108 [00:05<00:00, 608.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the same pipeline to the testing Dataset\n",
    "\n",
    "# Load the testing dataset\n",
    "test_df = preprocessor.load_dataset('../Preprocessing/STS_test.csv')\n",
    "\n",
    "# Normalize the text\n",
    "normal_test_df = preprocessor.remove_punctuation(test_df)\n",
    "normal_test_df = preprocessor.convert_to_lowercase(normal_test_df)\n",
    "normal_test_df = preprocessor.remove_empty_strings(normal_test_df)\n",
    "\n",
    "# Create 2 separate DataFrames, one without stopwords and the other also lemmatized\n",
    "sw_test_df = preprocessor.remove_stopwords(normal_test_df)\n",
    "lemmas_test_df = preprocessor.lemmatize(sw_test_df)\n",
    "\n",
    "# Group the 3 DataFrames into a Dictionary, with their \"names\"\n",
    "test_dfs = {'normal': normal_test_df, 'sw': sw_test_df, 'lemmas': lemmas_test_df}\n",
    "\n",
    "# Create the features DataFrame\n",
    "test_features_df = pd.DataFrame()\n",
    "\n",
    "# Greedy String Tiling\n",
    "compute_greedy_string_tiling(test_dfs, test_features_df)\n",
    "\n",
    "# Character n-gram Cosine Similarity\n",
    "compute_character_ngram_similarity(test_dfs, test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a CSV with the Normalized Gold Standard\n",
    "test_features_df['gs'] = test_df['gs'] / 5.0\n",
    "\n",
    "test_features_df.to_csv('test/stringFeatures_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Variable  Correlation\n",
      "0        normal_gst_5     0.571769\n",
      "1            sw_gst_5     0.666634\n",
      "2        lemmas_gst_5     0.666148\n",
      "3       normal_gst_10     0.526147\n",
      "4           sw_gst_10     0.560675\n",
      "5       lemmas_gst_10     0.560554\n",
      "6   normal_char_2gram     0.721553\n",
      "7       sw_char_2gram     0.687611\n",
      "8   lemmas_char_2gram     0.690158\n",
      "9   normal_char_3gram     0.651404\n",
      "10      sw_char_3gram     0.643363\n",
      "11  lemmas_char_3gram     0.647888\n",
      "12  normal_char_4gram     0.577053\n",
      "13      sw_char_4gram     0.635147\n",
      "14  lemmas_char_4gram     0.642041\n",
      "15  normal_char_5gram     0.503058\n",
      "16      sw_char_5gram     0.625407\n",
      "17  lemmas_char_5gram     0.634286\n",
      "18                 gs     1.000000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = {}\n",
    "for column in train_features_df.columns:\n",
    "    corr, _ = pearsonr(train_features_df[column], train_df['gs'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "# Convert the dictionary to a DataFrame for tabular representation\n",
    "correlation_table = pd.DataFrame(list(correlations.items()), columns=['Variable', 'Correlation'])\n",
    "\n",
    "print(correlation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Variable  Correlation\n",
      "0        normal_gst_5     0.557095\n",
      "1            sw_gst_5     0.627919\n",
      "2        lemmas_gst_5     0.623206\n",
      "3       normal_gst_10     0.460836\n",
      "4           sw_gst_10     0.460177\n",
      "5       lemmas_gst_10     0.460322\n",
      "6   normal_char_2gram     0.618557\n",
      "7       sw_char_2gram     0.605632\n",
      "8   lemmas_char_2gram     0.601264\n",
      "9   normal_char_3gram     0.561061\n",
      "10      sw_char_3gram     0.590396\n",
      "11  lemmas_char_3gram     0.586472\n",
      "12  normal_char_4gram     0.490803\n",
      "13      sw_char_4gram     0.580157\n",
      "14  lemmas_char_4gram     0.578780\n",
      "15  normal_char_5gram     0.423879\n",
      "16      sw_char_5gram     0.565421\n",
      "17  lemmas_char_5gram     0.565477\n",
      "18                 gs     1.000000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "correlations = {}\n",
    "for column in test_features_df.columns:\n",
    "    corr, _ = pearsonr(test_features_df[column], test_df['gs'])\n",
    "    correlations[column] = corr\n",
    "\n",
    "# Convert the dictionary to a DataFrame for tabular representation\n",
    "correlation_table = pd.DataFrame(list(correlations.items()), columns=['Variable', 'Correlation'])\n",
    "\n",
    "print(correlation_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
