{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 26.7MB/s]                    \n",
      "2024-12-10 20:05:47 INFO: Downloaded file to C:\\Users\\maric\\stanza_resources\\resources.json\n",
      "2024-12-10 20:05:47 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-12-10 20:05:49 INFO: File exists: C:\\Users\\maric\\stanza_resources\\en\\default.zip\n",
      "2024-12-10 20:05:56 INFO: Finished downloading models and saved to C:\\Users\\maric\\stanza_resources\n",
      "2024-12-10 20:05:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 26.5MB/s]                    \n",
      "2024-12-10 20:05:56 INFO: Downloaded file to C:\\Users\\maric\\stanza_resources\\resources.json\n",
      "2024-12-10 20:05:57 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-12-10 20:05:57 INFO: Using device: cpu\n",
      "2024-12-10 20:05:57 INFO: Loading: tokenize\n",
      "2024-12-10 20:05:57 INFO: Loading: mwt\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 20:06:01 INFO: Loading: pos\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 20:06:01 INFO: Loading: lemma\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 20:06:02 INFO: Loading: depparse\n",
      "c:\\Users\\maric\\Documents\\MASTERS\\PrimerSemestre\\IHLT\\FinalProject_versionantiguaconflictos\\IHLT\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-10 20:06:02 INFO: Done loading processors!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\maric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ast\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "from typing import List, Set\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "from nltk.chunk import RegexpParser\n",
    "import copy\n",
    "from math import log\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError  # Import WordNetError\n",
    "import pandas as pd\n",
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en')\n",
    "nlp_stanza = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse', tokenize_pretokenized=True)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "# Download required resource\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from Preprocessing.preprocessingUtils import TextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>['But', 'other', 'sources', 'close', 'to', 'th...</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Micron', 'has', 'declared', 'its', 'first', ...</td>\n",
       "      <td>['Micron', \"'s\", 'numbers', 'also', 'marked', ...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['The', 'fines', 'are', 'part', 'of', 'failed'...</td>\n",
       "      <td>['Perry', 'said', 'he', 'backs', 'the', 'Senat...</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>['The', 'American', 'Anglican', 'Council', ','...</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['The', 'tech-loaded', 'Nasdaq', 'composite', ...</td>\n",
       "      <td>['The', 'technology-laced', 'Nasdaq', 'Composi...</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...   \n",
       "1  ['Micron', 'has', 'declared', 'its', 'first', ...   \n",
       "2  ['The', 'fines', 'are', 'part', 'of', 'failed'...   \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...   \n",
       "4  ['The', 'tech-loaded', 'Nasdaq', 'composite', ...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  ['But', 'other', 'sources', 'close', 'to', 'th...  4.00  \n",
       "1  ['Micron', \"'s\", 'numbers', 'also', 'marked', ...  3.75  \n",
       "2  ['Perry', 'said', 'he', 'backs', 'the', 'Senat...  2.80  \n",
       "3  ['The', 'American', 'Anglican', 'Council', ','...  3.40  \n",
       "4  ['The', 'technology-laced', 'Nasdaq', 'Composi...  2.40  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_token_df = pd.read_csv('../Preprocessing/STS_train.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "train_token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>[But, other, sources, close, to, the, sale, sa...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Micron, has, declared, its, first, quarterly,...</td>\n",
       "      <td>[Micron, 's, numbers, also, marked, the, first...</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The, fines, are, part, of, failed, Republican...</td>\n",
       "      <td>[Perry, said, he, backs, the, Senate, 's, effo...</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>[The, American, Anglican, Council, ,, which, r...</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The, tech-loaded, Nasdaq, composite, rose, 20...</td>\n",
       "      <td>[The, technology-laced, Nasdaq, Composite, Ind...</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [But, other, sources, close, to, the, sale, sa...   \n",
       "1  [Micron, has, declared, its, first, quarterly,...   \n",
       "2  [The, fines, are, part, of, failed, Republican...   \n",
       "3  [The, American, Anglican, Council, ,, which, r...   \n",
       "4  [The, tech-loaded, Nasdaq, composite, rose, 20...   \n",
       "\n",
       "                                                   1    gs  \n",
       "0  [But, other, sources, close, to, the, sale, sa...   4.0  \n",
       "1  [Micron, 's, numbers, also, marked, the, first...  3.75  \n",
       "2  [Perry, said, he, backs, the, Senate, 's, effo...   2.8  \n",
       "3  [The, American, Anglican, Council, ,, which, r...   3.4  \n",
       "4  [The, technology-laced, Nasdaq, Composite, Ind...   2.4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the 2 first columns from strings to actual lists of strings\n",
    "\n",
    "n=len(train_token_df)\n",
    "\n",
    "train_df = pd.DataFrame(columns=['0','1','gs'], index=range(n))\n",
    "train_df.iloc[:, :2] = train_token_df.iloc[:, :2].map(ast.literal_eval)\n",
    "train_df.loc[:, 'gs'] = train_token_df.loc[:, 'gs']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TextPreprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Remove punctuation, convert to lowercase and remove empty strings\n",
    "train_df = preprocessor.remove_punctuation(train_df)\n",
    "train_df = preprocessor.convert_to_lowercase(train_df)\n",
    "train_df = preprocessor.remove_empty_strings(train_df)\n",
    "\n",
    "# Create the syntactic features data frame\n",
    "\n",
    "train_features_df=pd.DataFrame(columns=['translation_sim'], index=range(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: ['the', 'cat', 'sat']\n",
      "Translated Sentence: ['assis', 'assis', 'assis']\n",
      "\n",
      "Translation Probabilities:\n",
      "Translation probabilities for 'the':\n",
      "  the -> assis: 0.3333\n",
      "  the -> chat: 0.3333\n",
      "  the -> chien: 0.3333\n",
      "  the -> aboyé: 0.3333\n",
      "  the -> le: 0.3333\n",
      "  the -> tapis: 0.3333\n",
      "  the -> sur: 0.3333\n",
      "Translation probabilities for 'cat':\n",
      "  cat -> assis: 0.3333\n",
      "  cat -> chat: 0.3333\n",
      "  cat -> chien: 0.1429\n",
      "  cat -> aboyé: 0.1429\n",
      "  cat -> le: 0.3333\n",
      "  cat -> tapis: 0.1429\n",
      "  cat -> sur: 0.1429\n",
      "Translation probabilities for 'sat':\n",
      "  sat -> assis: 0.3333\n",
      "  sat -> chat: 0.3333\n",
      "  sat -> chien: 0.1429\n",
      "  sat -> aboyé: 0.1429\n",
      "  sat -> le: 0.3333\n",
      "  sat -> tapis: 0.1429\n",
      "  sat -> sur: 0.1429\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class MosesSMT:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize Moses Statistical Machine Translation system components\n",
    "        \"\"\"\n",
    "        # Translation probability table (word-level translation probabilities)\n",
    "        self.translation_prob_table = {}\n",
    "        \n",
    "        # Lexical translation probabilities\n",
    "        self.lexical_translation_probs = {}\n",
    "        \n",
    "        # Language model probabilities\n",
    "        self.language_model_probs = {}\n",
    "        \n",
    "        # Phrase translation probabilities\n",
    "        self.phrase_translation_probs = {}\n",
    "        \n",
    "        # Distortion probabilities (for word reordering)\n",
    "        self.distortion_probs = {}\n",
    "    \n",
    "    def train_translation_model(self, \n",
    "                                 parallel_corpus: List[Tuple[List[str], List[str]]], \n",
    "                                 num_iterations: int = 5):\n",
    "        \"\"\"\n",
    "        Train translation model using IBM Model 1 approach\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        parallel_corpus : List[Tuple[List[str], List[str]]]\n",
    "            Parallel corpus of source and target language sentence pairs\n",
    "        num_iterations : int\n",
    "            Number of EM (Expectation-Maximization) iterations for training\n",
    "        \"\"\"\n",
    "        # Initialize uniform translation probabilities\n",
    "        def initialize_translation_probabilities(parallel_corpus):\n",
    "            # Create unique vocabulary for source and target languages\n",
    "            src_vocab = set(word for src, _ in parallel_corpus for word in src)\n",
    "            tgt_vocab = set(word for _, tgt in parallel_corpus for word in tgt)\n",
    "            \n",
    "            # Initialize uniform probabilities\n",
    "            translation_probs = {}\n",
    "            for src_word in src_vocab:\n",
    "                translation_probs[src_word] = {\n",
    "                    tgt_word: 1.0 / len(tgt_vocab) \n",
    "                    for tgt_word in tgt_vocab\n",
    "                }\n",
    "            return translation_probs\n",
    "        \n",
    "        # Initial translation probability estimation\n",
    "        self.translation_prob_table = initialize_translation_probabilities(parallel_corpus)\n",
    "        \n",
    "        # EM Algorithm for refining translation probabilities\n",
    "        for _ in range(num_iterations):\n",
    "            # Expectation step: compute expected counts\n",
    "            expected_counts = {}\n",
    "            for src_sent, tgt_sent in parallel_corpus:\n",
    "                # Compute normalization\n",
    "                for src_word in src_sent:\n",
    "                    total_prob = sum(\n",
    "                        self.translation_prob_table[src_word].get(tgt_word, 0.0)\n",
    "                        for tgt_word in tgt_sent\n",
    "                    )\n",
    "                    \n",
    "                    # Update expected counts\n",
    "                    for tgt_word in tgt_sent:\n",
    "                        count = self.translation_prob_table[src_word].get(tgt_word, 0.0) / total_prob\n",
    "                        expected_counts[(src_word, tgt_word)] = count\n",
    "            \n",
    "            # Maximization step: update translation probabilities\n",
    "            for (src_word, tgt_word), count in expected_counts.items():\n",
    "                self.translation_prob_table[src_word][tgt_word] = count\n",
    "    \n",
    "    def decode(self, source_sentence: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decode source sentence using learned translation probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_sentence : List[str]\n",
    "            Input source language sentence to translate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List[str]\n",
    "            Translated target language sentence\n",
    "        \"\"\"\n",
    "        # Simple decoding strategy based on highest translation probability\n",
    "        decoded_sentence = []\n",
    "        for src_word in source_sentence:\n",
    "            # Find target word with highest translation probability\n",
    "            best_translation = max(\n",
    "                self.translation_prob_table.get(src_word, {}).items(),\n",
    "                key=lambda x: x[1],\n",
    "                default=(src_word, 1.0)\n",
    "            )[0]\n",
    "            decoded_sentence.append(best_translation)\n",
    "        \n",
    "        return decoded_sentence\n",
    "    \n",
    "    def compute_translation_probability(self, \n",
    "                                        source_word: str, \n",
    "                                        target_word: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute translation probability for a word pair\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_word : str\n",
    "            Word in source language\n",
    "        target_word : str\n",
    "            Word in target language\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Translation probability\n",
    "        \"\"\"\n",
    "        return self.translation_prob_table.get(source_word, {}).get(target_word, 0.0)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Simulated parallel corpus (source, target language pairs)\n",
    "    parallel_corpus = [\n",
    "        ([\"the\", \"cat\", \"sat\"], [\"le\", \"chat\", \"assis\"]),\n",
    "        ([\"on\", \"the\", \"mat\"], [\"sur\", \"le\", \"tapis\"]),\n",
    "        ([\"the\", \"dog\", \"barked\"], [\"le\", \"chien\", \"aboyé\"])\n",
    "    ]\n",
    "    \n",
    "    # Initialize and train Moses SMT model\n",
    "    moses_smt = MosesSMT()\n",
    "    moses_smt.train_translation_model(parallel_corpus)\n",
    "    \n",
    "    # Translate a sample sentence\n",
    "    source_sentence = [\"the\", \"cat\", \"sat\"]\n",
    "    translated_sentence = moses_smt.decode(source_sentence)\n",
    "    \n",
    "    print(\"Source Sentence:\", source_sentence)\n",
    "    print(\"Translated Sentence:\", translated_sentence)\n",
    "    \n",
    "    # Demonstrate translation probability computation\n",
    "    print(\"\\nTranslation Probabilities:\")\n",
    "    for src_word in source_sentence:\n",
    "        print(f\"Translation probabilities for '{src_word}':\")\n",
    "        for tgt_word, prob in moses_smt.translation_prob_table[src_word].items():\n",
    "            print(f\"  {src_word} -> {tgt_word}: {prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_tagging_unigrams</th>\n",
       "      <th>POS_tagging_bigrams</th>\n",
       "      <th>POS_tagging_trigrams</th>\n",
       "      <th>chunk_sim_p</th>\n",
       "      <th>chunk_sim_s</th>\n",
       "      <th>chunk_sim_o</th>\n",
       "      <th>total_sim_chunks</th>\n",
       "      <th>sim_dependencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POS_tagging_unigrams  POS_tagging_bigrams  POS_tagging_trigrams  \\\n",
       "0              0.702703             0.594595              0.514286   \n",
       "1              0.571429             0.421053              0.352941   \n",
       "2              0.500000             0.250000              0.090909   \n",
       "3              0.777778             0.764706              0.750000   \n",
       "4              0.307692             0.083333              0.000000   \n",
       "\n",
       "  chunk_sim_p chunk_sim_s chunk_sim_o total_sim_chunks sim_dependencies  \n",
       "0         NaN         NaN         NaN              NaN              NaN  \n",
       "1         NaN         NaN         NaN              NaN              NaN  \n",
       "2         NaN         NaN         NaN              NaN              NaN  \n",
       "3         NaN         NaN         NaN              NaN              NaN  \n",
       "4         NaN         NaN         NaN              NaN              NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the similarity \n",
    "\n",
    "train_features_df = pos_tagging_syntactic_ngrams(train_df, train_features_df)\n",
    "train_features_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IHLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
